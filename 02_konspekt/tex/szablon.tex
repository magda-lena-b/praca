\documentclass[a4paper,11pt,twoside]{report}
% KOMPILOWAĆ ZA POMOCĄ pdfLaTeXa, PRZEZ XeLaTeXa MOŻE NIE BYĆ POLSKICH ZNAKÓW

%-------------- DO: Kodowanie znakow, jezyk polski ----------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[MeX]{polski}
\usepackage[T1]{fontenc}
\usepackage[english,polish]{babel}
\usepackage{hyperref}



\usepackage{amsmath, amsfonts, amsthm, latexsym} % głównie symbole matematyczne, środowiska twierdzeń

\usepackage[final]{pdfpages} % inputowanie pdfa
%\usepackage[backend=bibtex, style=verbose-trad2]{biblatex}


%-------------- DO: Marginesy, akapity, interlinia ----------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
\usepackage[inner=20mm, outer=20mm, bindingoffset=10mm, top=25mm, bottom=25mm]{geometry}



\linespread{1.5}
\allowdisplaybreaks

\usepackage{indentfirst} % opcjonalnie; pierwszy akapit z wcięciem
\setlength{\parindent}{5mm}


%-------------- DO: Zywa pagina -----------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
% numery stron: lewa do lewego, prawa do prawego 
\fancyfoot[LE,RO]{\thepage} 
% prawa pagina: zawartość \rightmark do lewego, wewnętrznego (marginesu) 
\fancyhead[LO]{\sc \nouppercase{\rightmark}}
% lewa pagina: zawartość \leftmark do prawego, wewnętrznego (marginesu) 
\fancyhead[RE]{\sc \leftmark}

\renewcommand{\chaptermark}[1]{
\markboth{\thechapter.\ #1}{}}

% kreski oddzielające paginy (górną i dolną):
\renewcommand{\headrulewidth}{0 pt} % 0 - nie ma, 0.5 - jest linia


\fancypagestyle{plain}{% to definiuje wygląd pierwszej strony nowego rozdziału - obecnie tylko numeracja
  \fancyhf{}%
  \fancyfoot[LE,RO]{\thepage}%
  
  \renewcommand{\headrulewidth}{0pt}% Line at the header invisible
  \renewcommand{\footrulewidth}{0.0pt}
}

%-------------- DO: Naglowki rozdzialow ---------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
\usepackage{titlesec}
\titleformat{\chapter}%[display]
  {\normalfont\Large \bfseries}
  {\thechapter.}{1ex}{\Large}

\titleformat{\section}
  {\normalfont\large\bfseries}
  {\thesection.}{1ex}{}
\titlespacing{\section}{0pt}{30pt}{20pt} 
%\titlespacing{\co}{akapit}{ile przed}{ile po} 
    
\titleformat{\subsection}
  {\normalfont \bfseries}
  {\thesubsection.}{1ex}{}


%-------------- DO: Spis tresci -----------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
\def\cleardoublepage{\clearpage\if@twoside
\ifodd\c@page\else\hbox{}\thispagestyle{empty}\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}

% kropki dla chapterów
\usepackage{etoolbox}
\makeatletter
\patchcmd{\l@chapter}
  {\hfil}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill}
  {}{}
\makeatother

\usepackage{titletoc}
\makeatletter
\titlecontents{chapter}% <section-type>
  [0pt]% <left>
  {}% <above-code>
  {\bfseries \thecontentslabel.\quad}% <numbered-entry-format>
  {\bfseries}% <numberless-entry-format>
  {\bfseries\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}% <filler-page-format>

\titlecontents{section}
  [1em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{subsection}
  [2em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}
\makeatother

%-------------- DO: Spisy tabel i obrazkow ------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
\renewcommand*{\thetable}{\arabic{chapter}.\arabic{table}}
\renewcommand*{\thefigure}{\arabic{chapter}.\arabic{figure}}
%\let\c@table\c@figure % jeśli włączone, numeruje tabele i obrazki razem

%-------------- DO: Definicje, twierdzenia etc. -------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
\makeatletter
\newtheoremstyle{definition}%    % Name
{3ex}%                           % Space above
{3ex}%                           % Space below
{\upshape}%                      % Body font
{}%                              % Indent amount
{\bfseries}%                     % Theorem head font
{.}%                             % Punctuation after theorem head
{.5em}%                            % Space after theorem head, ' ', or \newline
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}%  % Theorem head spec (can be left empty, meaning `normal')
\makeatother
%-------------- DO: Polski ----------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
\theoremstyle{definition}
\newtheorem{theorem}{Twierdzenie}[chapter]
\newtheorem{lemma}[theorem]{Lemat}
\newtheorem{example}[theorem]{Przykład}
\newtheorem{proposition}[theorem]{Stwierdzenie}
\newtheorem{corollary}[theorem]{Wniosek}
\newtheorem{definition}[theorem]{Definicja}
\newtheorem{remark}[theorem]{Uwaga}

%-------------- DO: datkowe ---------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------

\usepackage{color}
%\usepackage{amssymb} %w pliku amssymb.sty zakomentowana 151i152 linijka: "%\DeclareMathSymbol{\lll}          {\mathrel}{AMSa}{"6E}"
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{subfigure} %zeby mozna bylo kilka grafik obok siebie...
\graphicspath{ {./imgs/} }
\usepackage{longtable}

\setcounter{secnumdepth}{4}
%\titleformat{\subsubsection}

\usepackage{multirow}

%-------------- DO: Dowod -----------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
%%\makeatletter
%\renewenvironment{proof}[1][\proofname]
%{\par
%  \vspace{-12pt}% remove the space after the theorem
%  \pushQED{\qed}%
%  \normalfont
%  \topsep0pt \partopsep0pt % no space before
%  \trivlist
%  \item[\hskip\labelsep
%        \sc
%    #1\@addpunct{:}]\ignorespaces
%}
%{%
%  \popQED\endtrivlist\@endpefalse
%  \addvspace{20pt} % some space after
%}
%
%\renewcommand{\qedhere}{\hfill \qedsymbol}
%\makeatother

%------------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
%--------------------- POCZATEK -----------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------

%--------------------- 00. USTAWIENIA UZYTKOWNIKA -----------------------------------------------------------
%------------------------------------------------------------------------------------------------------------

\newcommand{\tytul}{Zastosowanie metod przetwarzania języka naturlanego dla języka polskiego}
%\renewcommand{\title}{Missing value handling for classification tree creation and application}
%\newcommand{\type}{magisters} % magisters, licencjac
\newcommand{\supervisor}{dr~Grzegorz~Koloch}

\begin{document}
\sloppy

%\includepdf[pages=-]{titlepage}

%--------------------- 01. STRONA Z PODPISAMI AUTORA/AUTORÓW I PROMOTORA ------------------------------------
%------------------------------------------------------------------------------------------------------------
\thispagestyle{empty}\newpage
\null
\vfill
\begin{center}
\begin{tabular}[t]{ccc}
............................................. & \hspace*{100pt} & .............................................\\
podpis promotora & \hspace*{100pt} & podpis autora
\end{tabular}
\end{center}
%--------------------- 02. ABSTRAKTY -----------------------------------------------------------------------
%-----------------------------------------------------------------------------------------------------------


{
\begin{abstract}

\begin{center}
\tytul
\end{center}
Praca omawia wybrane zagadnienia z obszaru procesowania języka naturalnego skupiając się na możliwociach ich wykorzystania dla języka polskiego. W pracy umieszczono wyniki i analizę rezultatów zastosowania tych metod na przykładowym zbiorze danych tekstowych. \\
Zbiór danych wybrany do analiz to transkrypcje przemówień sejmowych z lat 1989 - 2019 r. \\
Praca skupia się na analizie i zastosowaniu następujących tematów: 
\begin{itemize}
\item modele n-gramowe,
\item modelowanie tematyczne,
\item analiza sentymentu.
\end{itemize}
Wykonano również analizy łączące tematy ze sobą oraz modele wykorzystujące wszystkie przygotowane informacje o korpusie.
\end{abstract}
}


%--------------------- 03. OSWIADCZENIE --------------------------------------------------------------------
%-----------------------------------------------------------------------------------------------------------


\null \hfill Warszawa, dnia ..................\\

\par\vspace{5cm}

\begin{center}
Oświadczenie
\end{center}

\indent Oświadczam, że pracę pod
tytułem ,,\tytul '', której promotorem jest \mbox{\supervisor}, wykonałam
samodzielnie, co poświadczam własnoręcznym podpisem.
\vspace{2cm}


\begin{flushright}
  \begin{minipage}{50mm}
    \begin{center}
      ..............................................

    \end{center}
  \end{minipage}
\end{flushright}

\thispagestyle{empty}
\newpage



%--------------------- 04. SPIS TRESCI ---------------------------------------------------------------------
%-----------------------------------------------------------------------------------------------------------
\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}


%--------------------- 05. ZASADNICZA CZESC PRACY ----------------------------------------------------------
%-----------------------------------------------------------------------------------------------------------
\null\thispagestyle{empty}\newpage
\pagestyle{fancy}
\pagenumbering{arabic}
% -> NOTE: JEŻELI Z POWODU DUŻEJ ILOŚCI STRON W SPISIE TREŚCI SIĘ NIE ZGADZA, TRZEBA ZMODYFIKOWAĆ RĘCZNIE
%\setcounter{page}{11} 

%\chapter*{Wstęp}
%\markboth{}{Wstęp}
%\addcontentsline{toc}{chapter}{Wstęp}
\chapter{Wstęp}
%\markboth{}{Wstęp}
%\addcontentsline{toc}{chapter}{Wstęp}
W ostatnich latach obszar przetwarzania języka naturalnego (z ang. \textit{NLP - natural language processing}) rozwija się bardzo intensywnie. Powstają nowe rozwiązania pozwalające na zaawansowane przetwarzanie i analizę nieustrukturyzowanych danych jakimi są dane tekstowe. Coraz bardziej zaawansowane są modele pozwalające na interpretację jak również generowanie tekstu. \\
Do niedawna możliwoci wykorzystania tych narzędzi w pracy z językiem polskim były ograniczone, lecz obecnie dostępnych jest już wiele pakietów oraz narzędzi, które znacznie ułatwiają zadania tego typu.

\section{Cel i zakres pracy}\label{miss_vs_class}

Celem niniejszej pracy jest przegląd narzędzi NLP dostępnych dla języka polskiego oraz analiza zastosowań niektórych z nich na przykładowym zbiorze danych. \\
Niniejsza praca skupia się na takich elementach NLP, które przy względnie niskim poziomie złożoności mogą przynieść interesujące rezultaty. Badano możliwości w zakresie prostego generowania tekstu, określania tematu wypowiedzi i jej sentymentu. Starano się połączyć te elementy podczas kontrukcji modeli, które na podstawie pewnych charakterystyk tekstu, potrafiłyby odgadnąć jego autora i określić jego cechy. 
Dane, na których przeprowadzono analizy zawierają transkrypcje wystąpień sejmowych z lat 1989 - 2019 r. Dołączono do nich podstawowe informacje o autorze, co pozwoliło rozszerzyć zakres analiz o budowę modeli predykcyjnych różnego typu.

\section{Przegląd rozdziałów}
W pierwszym rozdziale pracy zaprezentowano przegląd pojęć i zgadnień związanych z przetwarzaniem języka naturalnego, które pojawiają się w niniejszej pracy. \\

W drugim rozdziale pracy zaprezentowano krótki przegląd narzędzi, które mogą zostać wykorzystane podczas pracy z analizami języka polskiego. pakiety , embeddingi, morfeusz \\

W trzecim rozdziale opisano zbiór danych, na którym przeprowadzano kolejne analizy. Znajduje się tu opis przetworzeń, jakie były konieczne do poszczególnych zadań. Każde zagadnienie wymagało innego podejścia do korpusu i innego zestawu informacji. Przeprowadzono dwa etapy czyszczenia danych, lematyzację, wektoryzację oraz przypisanie sentymentu. \\

W rozdziale czwartym opisano rezultaty otrzymane podczas pracy z modelami n-gramowymi. Na potrzeby tych analiz napisano w języku python klasę dedykowaną modelom tego typu. Zaimplementowane metody pozwalały m.in. na 
\begin{itemize}
\item konstrukcję modeli o różnej złożoności i dla różnych wymiarów,
\item generowanie tesktu w opartciu o dany obiekt typu model,
\item kalkulację miary perplexity dla danych fragmentów tekstu
\item generowanie zdania-reprezentanta danego modelu.
\end{itemize}


%---------------------------Rozdzial 2
\chapter{Podstawowe elementy i zagadnienia związane z procesowaniem języka naturalnego}
Procesowanie języka naturalnego jest bardzo obszerną dziedziną data science. Pokrywa szeroką gamę zagadnień rozpoczynając od najprostszych typu przewidywanie kolejnego słowa przy pisaniu SMSów na telefonie, po interpretację tekstu oraz tłumaczenie maszynowe. To właśnie zagadnienia związane z tłumaczeniem były tematem pierwszych projektów w obszarze NLP już w latach pięćdziesiątych XX wieku. \\
Podstawowe elementy przygotowywania danych do analiz związanych z przetwarzaniem języka naturalnego to:

\begin{itemize}
 \item tokenizacja, czyli podział tekstu na segmenty, najczęściej pojedyncze słowa,
 \item stemming ma na celu obcięcie wszystkich przyrostków i przedrostów aby zbliżyć słowo do podstawowej postaci,
 \item lematyzacja to przypisanie do każdego słowa jego formy podstawowej, która go reprezentuje, 
 \item tworzenie wektorów własnościowych (word embeddings) w uproszczeniu będących wektorową reprezentacją znaczenia danego słowa.
\end{itemize}


%---------------------------Rozdzial 3
\chapter{Dane i środowiska wykorzystane do analiz}

\section{Źródło danych}
Zbiór na którym zostaną przeprowadzone analizy został pobrany z serwisu kaggle.com (\url{https://www.kaggle.com/paweladamczak/polish-politicians-speeches}). Zawiera transkrypcje przemówień polskich polityków  z lat 1989 – 2019 oraz profil każdego z mówców. \\
Zbiór powstał poprzez zaczytanie plików w formacie pdf, które można znaleźć w archiwum sejmowym pod adresem \url{http://www.sejm.gov.pl/Sejm9.nsf/page.xsp/archiwum}. Większość plików ma charakterystyczny układ, w którym każde wystąpienie podzielone jest na części opatrzone nagłówkiem wskazującego autora tekstu umieszczonego poniżej. Niestety nie dla wszystkich kadencji został zachowany ten układ, co przełożyło się na problemy z identyfikacją autorów dla części wystąpień. Zostało to opisane w rozdziale \ref{section:autor}.

\section{Format danych}
Dane zostały udostępnione w formie bazy danych SQL zawierającej dwie tabele. Pierwsza z nich zawiera szczegółowy profil każdego polityka. W tabeli występuje 2626 posłów oraz 68 różnych wskazań partii z 8 kadencji. Imię i nazwisko posła nie jest kluczem głównym, ponieważ mogą się one pojawiać wielokrotnie w różnych kadencja i z przypisaniem do różnych partii. Najistotniejsze informacje zawarte w tej tabeli przedstawiono w Tabeli \ref{t1}. \\

Druga tabela zawiera transkrypcje przemówień sejmowych. Najistotniejsze kolumny przedstawiono w Tabeli \ref{t2}. 
Tabela zawiera 272 321 wierszy, w tym 272 221 z niepustym polem \textit{speech\_raw}. Niektóre wiersze zawierają fragmenty tych samych wystąpień. Dzieje się tak w wypadku, gdy przemówienie było przerywane wypowiedziami innych mówców. Unikalnych wystąpień znajduje się z bazie ok. 19 tys. \\
Ponieważ dane były dostarczone w formie bazy SQL, część analiz wykonano w tym środowisku. Większość analiz na dalszym etapie wykonano w języku python.


\begin{table} \centering
\begin{tabular}{ |c|c| } 
 \hline
 Nazwa kolumny w zbiorze & Opis  \\ \hline
 full\_name & imię i nazwisko \\  \hline
 elected & data wybrania na posła \\ \hline
graduated\_school & ukończona szkoła/uczelnia \\ \hline
education\_level & wykształcenie \\ \hline
occupation & zawód \\ \hline
party\_section & partia \\ \hline
number\_of\_votes & liczba otrzymanych głosów \\ \hline
languages & znane języki \\ \hline
last\_party & ostatnia partia \\ \hline
\end{tabular}
\caption{Dane dotyczące autorów wystąpień}
\label{t1}
\end{table}

\begin{table}[h] \centering
\begin{tabular}{ |c|c| } 
 \hline
 Nazwa kolumny w zbiorze & Opis  \\ \hline
session\_number & numer sesji \\ \hline
date\_ & data wystąpienia \\ \hline
number\_ & numer porządku obrad  \\ \hline
speech\_title & tytuł wystąpienia \\ \hline
speech\_raw & tekst wystąpienia \\ \hline
\end{tabular}
\caption{Dane dotyczące wystąpień} \label{t2}
\end{table}


\section{Algorytm przypisania autora tekstu}
\label{section:autor}

Podczas przeglądania danych zidentyfikowano błędy w przypisaniu id autorów przemówień. Konsekwencją tego jest brak możliwości łatwego przypisania autora danego wystąpienia do jego tekstu. Błędy występowały dla większości wpisów w bazie i mimo tego, że sprawiały wrażenie systematycznej zmiany (poprawna wartość wydawała się bliska z kontekście porządku leksykograficznego), nie udało się ustalić algorytmu, który mógłby przypisać poprawne wartości w sposób automatyczny.  \\
W związku z powyższym przypisanie autora (\textbf{author\_final}) wykonano w następujących krokach (10\_ author\_final.sql):
\begin{enumerate}
\item Wiele wystąpień zaczyna się od schematu „Poseł Imię Nazwisko:”. Pierwsze występujące w takim kontekście imię i nazwisko przypisano jako dane autora z tekstu (kolumna author\_by\_text).
\item Przypisano imię i nazwisko autora w oparciu o id\_ z bazy (kolumna author\_by\_id).
\item Dla przypadków gdzie dla danego id, chociaż raz author\_by\_text jest równe author\_by\_id, przypisano te wartości jako author\_final.
\item Dla pozostałych przypadków sprawdzono jaka wartość author\_by\_text pojawia się najczęściej w obrębie danego id. Jeśli wartość była niepusta, została przypisana jako author\_final.
\item Pozostałe przypadki to takie, gdzie najczęściej pole author\_by\_text było puste, tj. w tekście wystąpienia nie występowało zestawienie „Poseł Imię Nazwisko:”. Wynika to z różnej postaci plików  transkrypcją – dla części z nich fragmenty pogrubione na poniższym obrazku nie znalazły się w tekście dostępnym w bazie.
\begin{figure}[h]
\includegraphics{screen_pdf}
\centering
\end{figure}
\item W trakcie analizy danych niektóre id były weryfikowanych ręcznie w oryginalnych plikach pdf. Wartości dla nich wprowadzono ręcznie w kodzie.
\end{enumerate}
Po zastosowaniu powyższych kroków w bazie brakowało przypisania author\_final dla 795 id (na 3828). Dla pojedynczych fragmentów wystąpień uzupełnienie wyniosło ok 83\%.


\section{Czyszczenie danych}
Wykonano następujące etapy czyszczenia danych: 
\begin{itemize}
 \item usunięto znaki specjalne 
 \item wykasowano tytuły przemówień pojawiające się na początku każdego fragmentu
 \item usunięty fragmenty tekstu w nawiasach (np. (Oklaski.), (Dzwonek.))
 \item Usunięto fragmenty identyfikujące mówcę, tj. fragment „Poseł Imię Nazwisko:”
\end{itemize}

Następnie przygotowano drugi zestaw tekstów, które oczyszczono jeszcze bardziej z fragmentów mało informacyjnych takich jak:
\begin{itemize}
 \item "Poseł Imię Nazwisko:"
 \item Panie Marszałku!
 \item Pani Marszałek!
 \item Wysoka Izbo!
 \item Panie Ministrze!
 \item Dziękuję bardzo.

\end{itemize}

Po tym etapie czyszczenia w bazie pozostało 272 217 fragmentów przemówień o niezerowej długości, z których 225 385 ma przypisanego autora. Oba zestawy tekstów z różnym poziomem oczyszczenia będą stosowane w różnych analizach. Przykładowo do konsttrukcji modeli n-gramowych wykorzystanoo oba zestawy danych. \\
Fragmenty tych samych wypowiedzi, zgrupowane po dacie, autorze i tytule zostały połączone.
Po usunięciu tekstów, które po wszystkich modyfikacjach stały się puste, w bazie zostało \textbf{158 885} tekstów z przypisanym autorem.


%--------------------------- rozdzial 4
\chapter{Wstępna analiza danych}

Dla tekstów z przypisanym autorem przeprowadzono wstępną analizę danych. Po usunięciu cyfr, znaków interpunkcyjnych oraz słów z listy polskich \textit{stopwords} analizowano częstości w celu weryfikacji i rozszerzenia listy \textit{stopwords}. W wyniku analizy dodanie do niej następujące elementy, ze względu na niską wartość informacyjną i ryzyko wprowadzenia zaburzeń do dalszych analiz:
\begin{itemize}
\item pkt \item art \item 'Wysoka Izbo!' \item 'Panie Ministrze!' \item 'Dziękuję bardzo.'
\end{itemize}

Po wykonaniu tego czyszczenia, w korpusie znajduje się nieco ponad 42 mln słów. Przed lematyzacją w korpusie występuje 367 716 różnych słów.\\
Najczęściej wypowiadający się posłowie znajdują się w Tabeli \ref{t41}. 

\begin{table}[h] \centering
\begin{tabular}{|c|c|c|}
\hline
author\_final & Liczba slów	& Liczba wypowiedzi \\\hline
Stanisław Stec	& 477862 & 	1368 \\\hline
Mirosław Pawlak	& 203379 & 	1134 \\\hline
Romuald Ajchler	& 401879 	& 1120 \\\hline
Jan Kulas	& 547001	& 1101 \\\hline
Andrzej Szlachta	& 242306	 &  982 \\
\hline
\end{tabular} \caption{Najczęściej wypowiadający się posłowie} \label{t41}
\end{table}

Posłowie z największą liczbą słów w wypowiedziach znajdują się w Tabeli \ref{t42}.\\\\\\\\


\begin{table}[h!] \centering
\begin{tabular}{|c|c|c|}
\hline
author\_final	& Liczba slów	& Liczba wypowiedzi \\\hline
Jan Kulas	& 	547001	& 	1101 \\\hline
Józef Zych	& 	514652	& 	878 \\\hline
Jerzy Jaskiernia	& 	481512	& 	658 \\\hline
Stanisław Stec	& 	477862	& 	1368 \\\hline
Tadeusz Tomaszewski	& 	412781	& 	917 \\\hline
\end{tabular} \caption{Posłowie z najwięszą liczbą słów} \label{t42}
\end{table}

Słów, które można określić jako „rzadkie”, tj. występujące w całym korpusie nie więcej niż 5 razy jest 226 599 (w tym 126382 występuje tylko jeden raz), czyli stanowią one istotną większość. Przykładowe słowa pojawiające się dokładnie jeden raz w całym korpusie:\\
 zaanektować, babette, opowiadaną, odejmowana, trawiona, kolektywistyczna, zagonów, troić, maćkowy, centralizowany.\\
Słowa pojawiające się najczęściej, to:
\begin{table}[h] \centering
\begin{tabular}{| c | c |}
\hline
słowo & liczba wystąpień \\ \hline
rząd & 95603 \\
projektu & 95657 \\
prawa &  96092 \\
chodzi & 99480 \\
panie & 108932 \\
projekt & 113847 \\
pytanie & 116217 \\
pracy & 118301 \\
komisji & 146153 \\
państwa & 148339\\
\hline
\end{tabular}
\caption{Słowa pojawiające się najczęściej w korpusie}
\end{table}

%------------------------------------------------ rozdział 5

\chapter{Modele n-gramowe}

Pierwszym elementem analizowanym na danym korpusie były modele n-gramowe. Takie modele bazują na statystykach występowania n-gramów w analizowanym korpusie i mogą służyć do przewidywania kolejnego elementu sekwencji, jak również do generowania nowych sekwencji w~oparciu o zaobserwowane zależności. Można budować zarówno modele oparte o słowa jak i inne elementy jak pojedyncze znaki czy sylaby. Sekwencje 1-gramowe nazywane są unigramami, 2-gramowe bigramami a 3-gramowe trigramami. Wraz ze wzrostem parametru \textit{n} wzrasta zdolność modelu do odzwierciedlania bardziej złożonych zależności pomiędzy słowami, ale istotnie rośnie zapotrzebowanie na dane do jego konstrukcji.
Przykładowo dla modelu opartego o słowa prawdopodobieństwo, że w naszej sekwencji, po słowie „praca” wystąpi słowo „zaliczeniowa” wynosi:
\begin{equation}
P(zaliczeniowa|praca) = \frac{cnt(praca\_ zaliczeniowa)}{cnt(praca \_\_\_)}
\end{equation}
Gdzie \_\_\_ oznaca dowolne słowo a liczniki „cnt” zostały wygenerowane na korpusie, na którym trenowano model.
W praktyce, aby uzyskać rozkłąd prawdowpodobieństwa, należy zastosować wygładzanie, które zaadresuje problem zwracania prawdopodobieństw dla słów, które nie znalazły się w oryginalnym korpusie.\\
Modele n-gramowe są wykorzystywane m.in. w rozpoznawaniu mowy, badania poprawności pisowni i tłumaczeniu maszynowym.\\
W niniejszej pracy skupimy się na:
\begin{itemize}
\item generowaniu treści,
\item szacowaniu prawdopodobieństwa pojawienia się okrs=eślonego zdania w wypowiedzi,
\item znajdowaniu \textit{zdania - reprezentanta} dla danego korpusu.
\end{itemize}

Modele powstawały w wersjch wersjach dla 1, 2 oraz 3-gramów. Dla obu opcji w pierwszym kroku przygotowano słownik zawierający wszystkie występujące kombinacje odpowiednich n-gramów ze słowami po nich następującymi. 
Aby uprościć i zautomatyzować konstrukcję modeli, przygotowano klasę ng\_models (ng\_models.py). Obiekt tej klasy jest modelem n-gramowym utworzonym zgodnie z przekazanymi parametrami n oraz corpus. Tworzenie modelu odbywa się z uwzględnieniem znaków specjalnych oznaczających początek i koniec zdania. Podczas jego konstrukcji następuje przetworzenie korpusu metodą \textit{process\_text}, która m.in. usuwa znaki specjalne, duże litery, wielokrotne spacje.

\section{Generowanie przemówień} \label{section:gen}

Generowanie przemówień polega na przypisaniu słowa początkowego (lub wybraniu losowego) a następnie w sposób wybieraniu słów kolejnych na bazie słownika prawdopodobieństw zbudowanego w poprzednim punkcie.
Wraz ze wzrostem n, rośnie zapotrzebowanie na dane do modelu. W przypadku gdy danych do uczenia jest zbyt mało, model będzie po prostu odtwarzał zdania, które pojawiały się w danych uczących. Z~drugiej strony takie modele potrafią generować sekwencje lepszej jakości, bardziej przypominające prawdziwe zdania, ponieważ zachowują więcej logicznych powiązań pomiędzy kolejnymi słowami.
Do zbudowania najprostszych modeli nie były wykorzystane żadne z pakietów związanych z NLP. 

Za generowanie wypowiedzi odpowiada metoda \textit{generate} klasy \textit{ng\_models} pozwalająca generować sekwencje zdań zadanej długości. Metoda zaczyna od losowego słowa z listy słów pojawiających się na początku zdań. Każde zdanie generowane jest oddzielnie do momentu aż zostanie wylosowany znacznik końca zdania, lub zostanie przekroczony parametr ograniczający długość zdania.\\
Przykładowe przemówienie dla modelu 1-gramowego wygenerowane z parametrami (steps=10, max\_sent=10):\\
\textit{Panie marszałku. Poza tym pracownikom projekt ordynacji podatkowej dla polski 1. Nie przekreślamy sprawę jasno sformułowane są niezgodne z dwiema. Jak zostanie zwrócony jako szef zespołu na wykreśleniu pkt. Boję się o swoich list to zasłużone dla strażaków. Chciałbym skupić wyłącznie do spożycia alkoholu oraz refundacji. Wysoka izbo. Jakie gwarancje zastawnika niż pobierających świadczenia z góry rozdzielanie. Swoich dzieci. W praktyce zostało wyartykułowane.} \\

Przykładowe przemówienie dla modelu 1-gramowego wygenerowane z parametrami (steps=10, max\_sent=20):\\
\textit{Natomiast takich uchwał dotyczących kwestii warunków prawnych w budowie. Szanowny panie marszałku. Pierwsza. W rozporządzeniu rady sądownictwa dopiero na swoich kompetencji i. To kolejny projekt i szczególnej państwowej służby ochrony państwa. W takiej potrzeby zwiększania środków europejskich demokratów formułuje wnioski. Szanowni państwo dzisiaj o szczególne znaczenie specjalnych jest wierną. Powiedzieliśmy do spółki na stworzenie nowej rezerwy zostanie skazany. Panie marszałku. Manipulowanie tą wersją projektu skłania do kilkuletnich zaniedbań i. Pierwszy pracę. Dopiero w związku z przeznaczeniem a przede wszystkim informacyjne. To proszę państwa dla poszczególnych powiatów oraz zniesienie ograniczeń. Przypuszczam że każda ze strony młodzieży oraz upoważnienie na. Później próbuje ogarnąć tej sprawie wyjścia naprzeciw wspomnę np. Można było. Potrzeba uchwalenia zaproponowanego przez radę ministrów. Jej do pana ministra tchórzewskiego byłego likwidatora majątku zagrożonego. Wprowadza tę izbę nie udało mi wiadomo że albo. Wysoki sejmie.}\\

Łatwo jest ocenić, że powyższe wypowiedzi nie pochodzą z prawdziwych wystąpień sejmowych. Możliwych kombinacji słów następujących po sobie jest zbyt dużo aby taki model mógł zachować logikę pomiędzy kolejnymi elementami zdania. Modele bigramowe powinny lepiej sobie z tym radzić.\\\\
Przykładowe przemówienie dla modelu 2-gramowego wygenerowane z parametrami (steps=10, max\_sent=10):\\
\textit{Mam kłopot z budowaniem nowych mieszkań w złym świetle respektowanie. To podchodzi pod administrację. Chodzi zapewne o wiele trudniejsza jest sytuacja w państwie członkowskim. Jeżeli obie strony uznały za obowiązujące zajmowanie przez 10 lat. Panowie osiągnęli a my słyszymy sto takich zgromadzeń. Jeżeli uzyskaliśmy technologie na warunkach partnerskich; wtedy jest ona nam. Do orzekania... Tyle jesteś samorządny o tyle obecnie średnio 133 osoby na. Przyjęcie kompromisowych rozwiązań łączących projekt rządowy nad którym dyskutujemy to. Jak kupuję jakiś towar.}\\

Przykładowe przemówienie dla modelu 2-gramowego wygenerowane z parametrami (steps=10, max\_sent=20):\\ \textit{Chciałbym wykreślić ze sprawozdania... Powoli zanika a mogła to być może wykorzystawszy także niezły poziom techniczno-technologiczny jest znacznie lepszy niż ten który złożył przysięgę. Jednak prenumerować przez pocztę przez swoich i dlatego nie ma powodu do jakichkolwiek sytuacji które mogą naruszać wolności wykonywania zawodu. Czy zrewaloryzowana ale też o rozruchu. Nie ustosunkował.właśnie do tego że komitet badań naukowych lub organizacji społecznej o czym mówił pan prezes socha był łaskaw pan. To przekształcane w pracowniczy program emerytalny jest instytucją samofinansującą się która jest w pełni popiera i w imieniu posłów z. Będziemy kontrolować waszą działalność trybunał stanu który decyzją sejmu termin ten wydaje się tu toczy będzie miała zastosowanie w postępowaniu. Nie ochronę uprawnień najemców i dzierżawców które już dzisiaj plagi społecznej. Mówię wewnętrzna organizacja tego ratownictwa także. Gdyby obecne napięcia między przedstawicielami ministerstwa skarbu czy po przyjęciu będzie miała jedno pytanie: co pani poruszyła są prawnie chronione.}\\

Widać, że w drugim modelu tekst bardziej przypomina realnie wystąpienia i prawdziwe zdania. Oczywiście w dalszym ciągu nie ma żadnych wątpliwości, że nie są to realne wypowiedzi, ale ich czytanie jest nieco przyjemniejsze niż dla poprzedniego modelu.
Podjęto również próbę zbudowania modelu 3-gramowego. Czas kalkulacji wydłużył się istotnie. Budowa słownika do modelu trwała prawie 30 minut.\\
Przykładowy tekst wygenerowany z parametrami (steps=10, max\_sent=10):\\ \textit{Czy elementem realizacji polityki gospodarczej. Już zadała to pytanie bo jest tajemnicą poliszynela unikają one płacenia miliardowych podatków i nadmiernie wykorzystują pracowników. I~marek belka nie budzi naszego zaufania. Dziękuję unii wolności za to że ma wygrać najlepszy. Chciałabym przytaczać w tej chwili problem bezrobocia mimo że ten dokument został odtajniony decyzją wiceministra spraw wewnętrznych i administracji i innym. Wiadomo są jedne z głównych czynników utrudniających udany proces integracji repatriantów z polskim społeczeństwem z obywatelami żeby przez zaniechania rządu nie. Przecież trudno się spodziewać tego że przy wyważeniu argumentów ograniczenie czasu zgłaszania weta też ma pewne koncepcje które należałoby wysokiej izbie. Połowa to mogą być grupy członkowskie. Wtedy ta nowelizacja rzeczywiście mogła wejść w życie na drugim etapie jego nowelizacji. Panie prokuratorze zwrócić uwagę że istnieje bardzo złe mniemanie o politykach i o polityce wobec cudzoziemców polityce imigracyjnej jest takim dzwonem.}\\

Zdania sprawiają wrażenie lepiej skonstruowanych niż dla poprzednich modeli. Jednak stosunkowo mały zbiór danych uczących sprawia, że model w dużym stopniu powtarza całe zdania ze zbioru uczącego.
Przyjrzyjmy się jak wygląda słownik dla niektórych 3-gramów występujący w powyższym tekście. \\

\includegraphics[scale=0.8]{s1}

\begin{figure}
\includegraphics[scale=0.8]{s2}
\end{figure}
\begin{figure}
\includegraphics[scale=0.8]{s3}\\
\includegraphics[scale=0.8]{s4}
\end{figure}


Z powyższego wynika, że do budowy dobrego modelu dla większych wartości \textit{n} dla języka polskiego potrzebne są dużo większe zestawy danych. Dla języków z prostszymi gramatykami, gdzie nie występuje tak wiele odmian słów, wymagania te mogą być niższe.


\section{Badanie prawdopodobieństwa wystąpienia danego zdania} \label{section:perp}
Dla modeli n-gramowych mamy możliwość przeanalizowania spójności danej sekwencji z modelem. Tj. jak prawdopodobne jest, że dane zdanie pochodzi z danego modelu. Można to szacować za pomocą wartości \textit{perplexity}, która generalnie służy do oceny jakości modelu n-gramowego na danych testowych. Dla pojedynczego zdania o licznie wyrazów \textit{N} możemy wyliczyć:

\begin{equation}
Perplexity=p(w_{test})^{-\frac{1}{N}}
\end{equation}
\begin{equation}
p(w_{test})=\prod_{i=1}^{N+1} p(w_{i} | w_{i-n+1}^{i-1})
\label{eq53} \end{equation}

Liczniki i indeksowanie są tak dopasowane aby uwzględniać pozycję początkową i końcową wyrazu w zdaniu poprzez uwzględnianiu w analizach dodatkowych tokenów oznaczających początek i koniec zdania.
Aby zaadresować kwestię zerowania się iloczynu w przypadku pojawienia się n-gramów spoza korpusu, na którym trenowano model, wprowadza się różne metody wygładzania tej miary. Tutaj zastosujemy „add-1 smoothing”, która modyfikuje kalkulację pojedynczych prawdopodobieństw w następujący sposób:

\begin{equation}
p(w_{i} | w_{i-n+1}^{i-1}) = \frac{c(w_{i-n+1}^i)+1}{c(w_{i-n+1}^{i-1}) +V}
\label{eq54} \end{equation}
Aby zapewnić, że otrzymane wartości będą poprawnym rozkładem prawdopodobieństwa, wartość \textit{V} musi być równa liczbie możliwych kontynuacji każdej sekwencji powiększonej o 1 ze względu na możliwy koniec zdania.\\

Za kalkulację tej wielkości odpowiada metoda \textit{perplexity} zaimplementowana dla klasy \textit{ng\_models}. Im mniejsza wartość perplexity, tym większa spójność zdania testowego z korpusem, na którym uczony był model.
Przykładowo dla modelu 2-gramowego z poprzedniego punktu otrzymujemy dla zdania \textit{"Dzisiejsze posiedzenie ma na celu omówienie projektu ustawy"}’ wartość 109 921 a dla zdania \textit{"Lwy to zwierzęta kotowate żyjące na sawannie"} 4 240 107.

\section{Jakość i porównywalność modeli budowanych na mniejszych korpusach}
Jak można zaobserować na wynikach otrzymanych w rozdziale \ref{section:gen}, jakość modelu rozumiana jakość zdolność do imitacji tekstu z korpusu poprawia się istotnie przy wzroście parametru \textit{n}. Wymaga to jednak korpusu o dużym rozmiarze. W tym rozdziale spróbujemy odpowiedzieć na pytanie, czy mniejsze korpusy pozwalają na budowę modeli o interesujących właściwościach. Skupimy się na określaniu i mierzeniu różnicy pomiędzy modelami, z wykorzystaniem miary wprowadzonej w rozdziale \ref{section:perp}.

\subsection{Modele dla pojedynczych mówców}
Ze względu na niedużą wielkość korpusu po podzieleniu na poszczególnych autorów, na potrzeby tej analizy konstruowano wyłącznie modele unigramowe. Zbudowano oddzielne modele dla 5 polityków z największą liczbą słów w danych (400 – 500 tys.) oraz dla trzech z istotnie mniejszą ilością słów (<100 tys.). Porównano wyniki dla kilku mniej lub bardziej prawdopodobnych zdań. Wyniki znajdują się na rysunku \ref{pic:51}. \\

Na szaro zaznaczone są osoby, ze stosunkowo dużym zbiorem danych uczących. Widać, że ma to istotny wpływ w sytuacjach gdy oceniane są zdania „mało prawdopodobne”. Wówczas w kalkulacji prawdopodobieństwa ciągu wyrazów pojawiają się bardzo małe wartości, co z kolei przekłada się na wysokie perplexity. Ogółem prawdopodobieństwa mają podobne trendy dla wszystkich autorów, ale warto zauważyć zaburzenie kolejności pomiędzy zdaniami „Demokracja jest najlepszym systemem politycznym” a „Zupełnie przypadkowy układ pięciu słów”. Niemniej cieszy, że sekwencja „lew czarownica i stara szafa” w każdym przypadku są opcją najmniej prawdopodobną.\\\\\\

\begin{figure}[h] 
\includegraphics[scale=1.3]{p1} 
\centering
\caption{Miara perplexity dla modeli budowanych per mówca} \label{pic:51}
\end{figure} 
Wykres \ref{pic:52} pokazuje rozrzut wartości zależny od modelu, który pokazuje, że przy tak małych zbiorach uczących, trudno jest porównywać wartości pomiędzy modelami.\\

\begin{figure}[h] 
\includegraphics[scale=0.8]{p2} 
\centering
\caption{Wizualizacja miary perplexity dla modeli budowanych per mówca} \label{pic:52}
\end{figure}
Niemniej przeglądając słowniki dla poszczególnych modeli budowanych w ten sposób, widać, że cechuje je duża przypadkowość. Zbyt wiele n-gramów nie będzie się w nich odnajdywać i będą kontrybuowały do miary z tą samą, najniższą wartością. Zdecydowanie też miara ta nie nadaje się do porównywania  modeli budowanych na korpusach o istotnie różnej liczności.

\subsection{Modele dla partii}
Ponieważ modele budowane dla pojedynczych osób wydawały się budowane na zbyt małych korpusach, powtórzono analizy grupując autorów wg najnowszego przypisania do partii (SQL). Liczności słów w korpusie dla głównych partii wyglądają następująco:
\begin{center}
\begin{tabular}{|c|c|c|} 
\hline
partia & liczba słów & liczba wypowiedzi \\ \hline
Prawo i Sprawiedliwość & 12 409 825 & 31 630 \\ \hline
Sojusz Lewicy Demokratycznej & 13 143 036 & 27 129 \\ \hline
Platforma Obywatelska & 9 438 332 & 24 977 \\ \hline
Polskie Stronnictwo Ludowe & 8 256 113 & 17 851 \\ \hline
posłowie niezrzeszeni & 3 735 227 & 7 358 \\ \hline
Akcja Wyborcza Solidarność & 3 304 864 & 5 642 \\ \hline
Samoobrona & 2 027 951 & 4 407 \\ \hline
\end{tabular}
\end{center}
Wartości są istotnie wyższe, oczekiwane są więc stabilniejsze wyniki. \\
Zweryfikowano wartości perplexity na tym samym zbiorze zdań testowych. Wyniki pokazano na rysunku \ref{pic:53}. \\\\
\begin{figure}[h] 
\includegraphics[scale=0.6]{p3} 
\centering
\caption{Miara perplexity dla modeli budowanych per partia} \label{pic:53}
\end{figure} 
\begin{figure}[h] 
\includegraphics[scale=0.6]{p4} 
\centering
\caption{Wizualizacja miary perplexity dla modeli per partia} \label{pic:54}
\end{figure}

Na wykresie \ref{pic:54} porównano wyniki dla modeli unigramowych i bigramowych.
Można zauważyć, że dla modelu 1-gramowego wartości perplexity są dużo niższe co wynika m.in. z tego, że łatwiej trafić na wyrażenie dwuwyrazowe, które się w modelu pojawia niż 3 wyrazowe. Na pewno widać większą stabilność wyników. Porównywanie konkretnych wartości pomiędzy modelami zdecydowanie ma więcej sensu.



\section{Budowa zdań w oparciu o maksymalizację prawdopodonieństwa}
Poniższe analizy wykonano na tekście oczyszczonym z najczęściej pojawiających się, mało informacyjnych fraz typu „Panie Marszałku!”. \\
Znając prawdopodobieństwa następowania kolejnych słów po sobie, można spróbować zbudować zdanie minimalizujące wartość perplexity, tj. maksymalizujące prawdopodobieństwo „zobaczenia go w modelu”. W tym celu wyposażono klasę \textit{ng\_models} o metodę \textit{mprob\_sent}. Celem tego ćwiczenia było zbadanie możliwości otrzymania zdania, które byłoby najbardziej charakterystyczne dla danego korpusu, czyli można by je określić jako dewizę związaną z danym tematem, osobą czy też grupą. \\
Metoda zakłada pobranie pierwszego słowa zdania od użytkownika a następnie wybieraniu kolejnych słów wg prawdopodobieństw w modelu. Czyli jeśli słowem początkowym jest słowo „poseł”, metoda sprawdzi jakie n-gramy pojawiały się w modelu zaczynające się od słowa poseł i wybierze najczęstszy. Przykładowe najczęstsze następniki słowa „poseł” w modelu 1-gramowym dla wybranego mówcy.\\

\begin{figure}[h] 
\includegraphics[scale=0.8]{m1} 
\centering
\end{figure}
 
Niestety okazuje się, że taki algorytm bardzo łatwo wpada w pętle. Przykładowe zdania otrzymane w wyniku jego działania to:\\
\textit{Poseł sprawozdawca mówił o tym że w tym że w tym że w tym że w tym że w tym że w tym że w tym że w tym że w tym.} \\
\textit{Ja to jest to jest to jest to jest to jest to jest to jest to jest to jest to jest to jest to jest to jest to jest to jest to.}\\
Powód jest zobrazowany na poniższym obrazku.
\begin{figure}[h] 
\includegraphics[scale=0.8]{m2} 
\centering
\end{figure}

W takim przypadku, jeśli model trafi w pewnym momencie na słowo „to”, zapętli się i będzie powtarzał 2-gram „to jest”. \\
Jednym z możliwych rozwiązań jest zakaz powtarzania słów w obrębie jednego zdania. W przypadku trafienia na słowo, które już się pojawiło w trakcie generowania, wybieramy kolejne wg częstości pojawiania się. Kończymy generowanie, gdy sytuacja powtarza się zbyt często. Wówczas otrzymujemy np.:\\
\textit{Poseł sprawozdawca mówił o tym że w tej ustawy.}\\
\textit{Ja to jest bardzo ważne.}

W przypadku, gdy do metody nie zostanie przekazane pierwsze słowo, generowanie rozpocznie się od słowa, które najczęściej pojawia się jako pierwsze w zdaniach w danym korpusie. W poniższej tabeli umieszczono porównanie wyników dla różnych osób w zależności od słowa początkowego.\\

\begin{figure}[h] 
\includegraphics[scale=1.1]{m3} 
\centering \caption{Zdania wygenerowane w oparciu o maksymalizację prawdopodobieństwa - politycy}
\end{figure}

Niestety wyniki są do siebie bardzo podobne. Wygląda na to, że mimo rozróżnienia na poszczególnych mówców, w modelach zostały odzwierciedlone zasady ogólne języka. 
Długość otrzymanych zdań jest bezpośrednią pochodną skłonności do zapętlania się słów w danym korpusie. Widać, że dla większości najczęstszym słowem jest „w”.\\
Jako alternatywną metodę, która mogłaby wpłynąć na „urozmaicenie” generowanych zdać, rozważano usunięcie z korpusu tzw. \textit{stopwords}. To jednak mogłoby generować zdania nienaturalne w swojej gramatyce oraz zupełnie już pozbawione logiki nawet w horyzoncie kilku słów. Testowano również wykorzystanie modeli bardziej złożonych, tj. trigramowych, niestety te najczęściej cytowały dosłownie jedno ze zdań korpusu.\\
Również generowanie najbardziej prawdopodobnego zdania z modeli dla partii przyniosło rozczarowujące rezultaty. Okazuje się, że dla tego zadania zwiększony rozmiar korpusu powoduje, że modele 1-gramowe stają się zbyt podobne. Generowane zdania były prawie identyczne. \\

\begin{figure}[h] 
\includegraphics[scale=1.1]{m4} 
\centering \caption{Zdania wygenerowane w oparciu o maksymalizację prawdopodobieństwa - partie (1-gram)}
\end{figure}

Dla modeli 2-gramowych zdania są nieco bardziej zróżnicowane, ponieważ im dłuższe są n-gramy, na których opiera się model, tym mniejsza szansa, że będą się powtarzać w różnych korpusach równie często. Dalej jednak są dość podobne. \\
\begin{figure}[!ht] 
\includegraphics[scale=1.1]{m5} 
\centering \caption{Zdania wygenerowane w oparciu o maksymalizację prawdopodobieństwa - partie (2-gram)}
\end{figure}

Konstrukcja	ciągów wyrazów może zatem mieć więcej zastosowań w przypadkach, gdy jednocześnie spełniony jest warunek dotyczący większej ilości słów w korpusie i zadane są odpowiednie warunki brzegowe pozwalające na zróżnicowanie wyników.

\chapter{Dalszy processing danych}
Do kolejnych etapów analiz konieczne było rozszerzenie zakresu danych. Na danych wykonano lematyzację, wektoryzację oraz dodano oznaczenie sentymentu.

\section{Lematyzacja} \label{section:lem}
Do tego etapu pracy z danymi wykorzystano pakiet pythonowy stanza \url{https://stanfordnlp.github.io/stanza/index.html#about}. Jest to pakiet, który może być wykorzystywany m.in. do tokenizacji tekstu, lematyzacji słów, określania części mowy oraz zadań z obszaru NER (Named Entity Recognition).  Modele w pakiecie obejmują ponad 70 języków, w tym polski. Bazują na projekcie Universal Dependencies, którego celem jest spójny opis gramatyk dla różnych języków, który jest rozwijany w celu trenowania rozwiązań dla wielu języków jednocześnie.
W niniejszej pracy wykorzystano modele oparte o korpus LFG, który zawiera 17 246 zdań i 130 967 tokenów. Zawiera oznaczenia 15 części mowy: ADJ, ADP, ADV, AUX, CCONJ, DET, INTJ, NOUN, NUM, PART, PRON, PROPN, PUNCT, SCONJ, VERB.\\
Przykładowe wywołanie dla zdania „Pisanie pracy zaliczeniowej":\\
      id: 1,\\
      text: Pisanie,\\
      lemma: pisać,\\
      upos: NOUN,\\
      xpos: ger:sg:nom:n:imperf:aff,\\
      feats: Aspect=Imp|Case=Nom|Gender=Neut|Number=Sing|Polarity=Pos|VerbForm=Vnoun,\\
      head: 0,\\
      deprel: root,\\
      misc: start\_char=0|end\_char=7\\\\

      id: 2,\\
      text: pracy,\\
      lemma: praca,\\
      upos: NOUN,\\
      xpos: subst:sg:gen:f,\\
      feats: Case=Gen|Gender=Fem|Number=Sing,\\
      head: 1,\\
      deprel: nmod,\\
      misc: start\_char=8|end\_char=13\\

      id: 3,\\
      text: zaliczeniowej,\\
      lemma: zaliczeniowy,\\
      upos: ADJ,\\
      xpos: adj:sg:gen:f:pos,\\
      feats: Case=Gen|Degree=Pos|Gender=Fem|Number=Sing,\\
      head: 2,\\
      deprel: amod,\\
      misc: start\_char=14|end\_char=27\\

Bazując na powyższym pakiecie oraz liście polskich stopwords, przygotowano nową wersję tesktów przemówień, która: \begin{itemize}
\item nie zawiera znaków interpunkcyjnych,
\item składa się z lematów słów
\item nie zawiera słów z listy stopwords (lista rozszerzona o \textit{zł, pkt, art, ustawy, r}).
\end{itemize}

Po wykonaniu lematyzacji przemówień, liczba różnych słów w korpusie spadła z 367 tys. do 152 755 czyli ponad o połowę. Nieco ponad 101 tys. słów występuje mniej niż 5 razy.
Najczęściej występujące słowa przedstawione są w tabeli \ref{tab:najcz}.
\begin{table}[h] \centering
\begin{tabular}{|c|c|} 
\hline
'prawo' & 221 153 \\ \hline
 'komisja' & 227 835 \\ \hline
 'minister' & 236 987 \\ \hline
 'móc' & 241 134 \\ \hline
 'polski' & 245 334 \\ \hline
 'państwo' & 261 503 \\ \hline
 'projekt' & 310 992 \\ \hline
 'rok' & 392 826 \\ \hline
 'mieć' & 521 144 \\ \hline
 'ustawa' & 531 702 \\ \hline \end{tabular} \caption{Najczęściej występujące słowa po lematyzacji} \label{tab:najcz}
\end{table}

%-------------------------------------------------- przypisanie sentymentu
\section{Przypisanie sentymentu} \label{section:sent}







%--------------------------------------------------modelowanie tematyczne

\chapter{Modelowanie tematyczne}



Modelowanie tematyczne (ang. \textit{topic modeling}) jest dziedziną, która skupia się na wykrywaniu tematów w zbiorze dokumentów. Bazuje na stowarzyszeniu określonych słów z tymi tematami. To zagadnienie może występować jako przykład uczenia nadzorowanego. Będzie tak w przypadku, gdy posiadamy zestaw dokumentów z przypisanymi do nich tematami. Wówczas staramy się zbudować model, który nauczy się rozpoznawać te tematy w nowych dokumentach. Z uczeniem nienadzorowanym mamy do czynienia w przypadku, gdy dopiero chcemy odkryć strukturę danego zestawu dokumentów. Nie wiemy jakie tematy mogą się tam pojawiać i stosujemy różne narzędzia statystyczne aby je zidentyfikować.
Jednym z podejść, które można zastosować w takiej analizie jest schemat \textit{tf-idf} (od ang. \textit{term frequency – inverse document frequency}). Polega on na redukcji wymiarowości zbioru danych poprzez konstrukcję macierzy, w której dla każdego dokumentu w~zbiorze przypisujemy wektor bazujący na częstościach występowania w nim określonych słów. Wartość TF-IDF oblicza się ze wzoru:

\begin{equation} 
(tf -idf)_{i,j} =tf_{i,j} \times idf_{i},
\end{equation}
gdzie $tf_{i, j}$ oznacza „term frequency” i wyraża się wzorem:
\begin{equation}
tf_{i,j} =\frac {n_{i,j}}{\sum _{k}n_{k,j}},
\end{equation}
gdzie: $n_{i,j}$ jest liczbą wystąpień słowa $w_{i}$ w dokumencie $d_{j}$ a mianownik jest sumą liczby wystąpień wszystkich słów w dokumencie $d_{j}$. $idf_{i}$ to „inverse document frequency” wyrażana wzorem:
\begin{equation}
idf_{i} =\log \frac {|D|}{|\{d:w_{i}\in d\}|},
\end{equation}
gdzie: \\
$|D|$ – liczba dokumentów w korpusie,\\
$|\{d:w_{i}\in d\}|$ – liczba dokumentów zawierających przynajmniej jedno wystąpienie danego słowa.\\
Otrzymujemy w ten sposób wektory równych długości, które można wykorzystać do analizy podobieństwa dokumentów. Algorytm ten jest stosowany m.in. w wyszukiwarkach internetowych oraz systemach antyplagiatowych. \\
Aby zaadresować potrzebę jeszcze większej redukcji wymiarów, pojawiły się inne metody opisywania dokumentów. Jedną z popularniejszych jest latent Dirichlet allocatioc (LDA) przedstawiona w \cite{LDA}. Zakłada, że każdy dokument jest mieszanką różnych ukrytych (\textit{latent}) tematów i dla każdego tematu istnieje rozkład prawdopodobieństwa słów w nim występujących.\\
Proces generatywny dla dokumentów, zgodnie z LDA wygląda w następujący sposób (z pewnymi uproszczeniami):
\begin{enumerate}
\item Wybierz $N \sim Poisson(\xi)$.
\item Wybierz $\theta \sim Dir(a)$.
\item Dla każdego z $N$ słów $w_n$:
\begin{enumerate}
	\item Wybierz temat $z_n \sim$  Multinomial($\theta$).
	\item Wybierz słowo $w_n$ z rozkładu wielomianowego zależnego od tematu $z_n$, tj. $p(w_n|z_n;\beta)$.
\end{enumerate} \end{enumerate}
Zmienną $\theta$ należy utożsamiać z pewnym podziałem dokumentu na tematy. Przykładowo jeśli rozważamy trzy tematy, może ona przybrać wartość (0.5, 0.2, 0.3). Oznacza to, że połowa słów powinna być związania z tematem nr 1, 20\% z tematem nr 3 i 30\% z tematem 3. Gęstość $k$-wymiarowej zmiennej losowej o rozkłądzie Dirichleta opisuje wzór:
\begin{equation}
p(\theta|\alpha) = \frac {\Gamma (\sum_{i=1}^{k}\alpha_i)} {\prod_{i=1}^{k}\Gamma(\alpha_i)} \theta_1^{\alpha_1 -1} \dots \theta_k^{\alpha_k -1},
\end{equation}
gdzie $\alpha$ jest $k$-wymiarowym wektorem z parametrami $\alpha_i$>0, a $\Gamma$ oznacza funkcję Gamma.\\
Wówczas prawdopodobieństwo całego korpusu $D$ o liczbie dokumentów $M$ wynosi:

\begin{equation}
p(D|\alpha,\beta) = \prod_{d=1}^M \int p(\theta_d |\alpha) \left(\prod_{n=1}^{N_d} \sum_{z_{dn}} p(z_{dn}|\theta_d)p(w_{db}|z_{dn}, \beta) \right) d\theta_d.
 \end{equation}
Warto zauważyć, że parametry $\alpha$ i $\beta$ są właściwe dla całego korpusu. Parametr $\theta_d$ jest losowany dla każdego dokumentu, a zmienne $z_{dn}$ i $w_{dn}$ występują na poziomie słów.

Wybór optymalnej liczby tematów:
https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#17howtofindtheoptimalnumberoftopicsforlda
\section{Author-topic model}



%--------------------- 06. BIBLIOGRAFIA ---------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
% Bibliografia leksykograficznie wg nazwisk autorów
% Dla ambitnych - można skorzystać z BibTeX-a

\begin{thebibliography}{25}%jak ktoś ma więcej książek, to niech wpisze większą liczbę
% \bibitem[numerek]{referencja} Autor, \emph{Tytuł}, Wydawnictwo, rok, strony
% cytowanie: \cite{referencja1, referencja 2,...}
\bibitem{BH} Peter Hancox, \textit{A brief history of Natural Language Processing}, \url{https://www.cs.bham.ac.uk/~pjh/sem1a5/pt1/pt1_history.html}, dostęp 14.05.2020r.
\bibitem{NRU} National Research University Higher School of Economics, \textit{Natural Language Processing}, \url{https://www.coursera.org/learn/language-processing/home/info}
\bibitem{LDA} David M. Blei, Andrew Y. Ng and Michael I. Jordan, \textit{Latent Dirichlet Allocation}, Journal of Machine Learning Research 3 (2003) 993-1022
%\bibitem{JRQ} J.R. Quinlan, \textit{Induction of Decision Trees}, Machine Learning 1: Kluwer Academic Publishers, Boston 1986

\end{thebibliography}

\thispagestyle{empty}
\pagenumbering{gobble}


% ----- 8. Spis rysunków - 
\listoffigures
\thispagestyle{empty}
%Jak nie występują, usunąć.


% ------------ 9. Spis tabel - jak wyżej ------------------
\renewcommand{\listtablename}{Spis tabel}
\listoftables
\thispagestyle{empty}
%Jak nie występują, usunąć.


% 10. Spis załączników - 

%\chapter*{Spis załączników}
%\begin{enumerate}
%\item kody\autor.sql

%\end{enumerate}
\thispagestyle{empty}


\end{document}
