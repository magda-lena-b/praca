\documentclass[a4paper,11pt,twoside]{report}
% KOMPILOWAĆ ZA POMOCĄ pdfLaTeXa, PRZEZ XeLaTeXa MOŻE NIE BYĆ POLSKICH ZNAKÓW

%-------------- DO: Kodowanie znakow, jezyk polski ----------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[MeX]{polski}
\usepackage[T1]{fontenc}
\usepackage[english,polish]{babel}
\usepackage{hyperref}



\usepackage{amsmath, amsfonts, amsthm, latexsym} % głównie symbole matematyczne, środowiska twierdzeń

\usepackage[final]{pdfpages} % inputowanie pdfa
%\usepackage[backend=bibtex, style=verbose-trad2]{biblatex}


%-------------- DO: Marginesy, akapity, interlinia ----------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
\usepackage[inner=20mm, outer=20mm, bindingoffset=10mm, top=25mm, bottom=25mm]{geometry}



\linespread{1.5}
\allowdisplaybreaks

\usepackage{indentfirst} % opcjonalnie; pierwszy akapit z wcięciem
\setlength{\parindent}{5mm}


%-------------- DO: Zywa pagina -----------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
% numery stron: lewa do lewego, prawa do prawego 
\fancyfoot[LE,RO]{\thepage} 
% prawa pagina: zawartość \rightmark do lewego, wewnętrznego (marginesu) 
\fancyhead[LO]{\sc \nouppercase{\rightmark}}
% lewa pagina: zawartość \leftmark do prawego, wewnętrznego (marginesu) 
\fancyhead[RE]{\sc \leftmark}

\renewcommand{\chaptermark}[1]{
\markboth{\thechapter.\ #1}{}}

% kreski oddzielające paginy (górną i dolną):
\renewcommand{\headrulewidth}{0 pt} % 0 - nie ma, 0.5 - jest linia


\fancypagestyle{plain}{% to definiuje wygląd pierwszej strony nowego rozdziału - obecnie tylko numeracja
  \fancyhf{}%
  \fancyfoot[LE,RO]{\thepage}%
  
  \renewcommand{\headrulewidth}{0pt}% Line at the header invisible
  \renewcommand{\footrulewidth}{0.0pt}
}

%-------------- DO: Naglowki rozdzialow ---------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
\usepackage{titlesec}
\titleformat{\chapter}%[display]
  {\normalfont\Large \bfseries}
  {\thechapter.}{1ex}{\Large}

\titleformat{\section}
  {\normalfont\large\bfseries}
  {\thesection.}{1ex}{}
\titlespacing{\section}{0pt}{30pt}{20pt} 
%\titlespacing{\co}{akapit}{ile przed}{ile po} 
    
\titleformat{\subsection}
  {\normalfont \bfseries}
  {\thesubsection.}{1ex}{}


%-------------- DO: Spis tresci -----------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
\def\cleardoublepage{\clearpage\if@twoside
\ifodd\c@page\else\hbox{}\thispagestyle{empty}\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}

% kropki dla chapterów
\usepackage{etoolbox}
\makeatletter
\patchcmd{\l@chapter}
  {\hfil}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill}
  {}{}
\makeatother

\usepackage{titletoc}
\makeatletter
\titlecontents{chapter}% <section-type>
  [0pt]% <left>
  {}% <above-code>
  {\bfseries \thecontentslabel.\quad}% <numbered-entry-format>
  {\bfseries}% <numberless-entry-format>
  {\bfseries\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}% <filler-page-format>

\titlecontents{section}
  [1em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{subsection}
  [2em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}
\makeatother

%-------------- DO: Spisy tabel i obrazkow ------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
\renewcommand*{\thetable}{\arabic{chapter}.\arabic{table}}
\renewcommand*{\thefigure}{\arabic{chapter}.\arabic{figure}}
%\let\c@table\c@figure % jeśli włączone, numeruje tabele i obrazki razem

%-------------- DO: Definicje, twierdzenia etc. -------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
\makeatletter
\newtheoremstyle{definition}%    % Name
{3ex}%                           % Space above
{3ex}%                           % Space below
{\upshape}%                      % Body font
{}%                              % Indent amount
{\bfseries}%                     % Theorem head font
{.}%                             % Punctuation after theorem head
{.5em}%                            % Space after theorem head, ' ', or \newline
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}%  % Theorem head spec (can be left empty, meaning `normal')
\makeatother
%-------------- DO: Polski ----------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
\theoremstyle{definition}
\newtheorem{theorem}{Twierdzenie}[chapter]
\newtheorem{lemma}[theorem]{Lemat}
\newtheorem{example}[theorem]{Przykład}
\newtheorem{proposition}[theorem]{Stwierdzenie}
\newtheorem{corollary}[theorem]{Wniosek}
\newtheorem{definition}[theorem]{Definicja}
\newtheorem{remark}[theorem]{Uwaga}

%-------------- DO: datkowe ---------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------

\usepackage{color}
%\usepackage{amssymb} %w pliku amssymb.sty zakomentowana 151i152 linijka: "%\DeclareMathSymbol{\lll}          {\mathrel}{AMSa}{"6E}"
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{subfigure} %zeby mozna bylo kilka grafik obok siebie...
\graphicspath{ {./imgs/} }
\usepackage{longtable}

\setcounter{secnumdepth}{4}
%\titleformat{\subsubsection}

\usepackage{multirow}

%-------------- DO: Dowod -----------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
%%\makeatletter
%\renewenvironment{proof}[1][\proofname]
%{\par
%  \vspace{-12pt}% remove the space after the theorem
%  \pushQED{\qed}%
%  \normalfont
%  \topsep0pt \partopsep0pt % no space before
%  \trivlist
%  \item[\hskip\labelsep
%        \sc
%    #1\@addpunct{:}]\ignorespaces
%}
%{%
%  \popQED\endtrivlist\@endpefalse
%  \addvspace{20pt} % some space after
%}
%
%\renewcommand{\qedhere}{\hfill \qedsymbol}
%\makeatother

%------------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
%--------------------- POCZATEK -----------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------

%--------------------- 00. USTAWIENIA UZYTKOWNIKA -----------------------------------------------------------
%------------------------------------------------------------------------------------------------------------

\newcommand{\tytul}{Zastosowanie metod przetwarzania języka naturlanego dla języka polskiego}
%\renewcommand{\title}{Missing value handling for classification tree creation and application}
%\newcommand{\type}{magisters} % magisters, licencjac
\newcommand{\supervisor}{dr~Grzegorz~Koloch}

\begin{document}
\sloppy

%\includepdf[pages=-]{titlepage}

%--------------------- 01. STRONA Z PODPISAMI AUTORA/AUTORÓW I PROMOTORA ------------------------------------
%------------------------------------------------------------------------------------------------------------
\thispagestyle{empty}\newpage
\null
\vfill
\begin{center}
\begin{tabular}[t]{ccc}
............................................. & \hspace*{100pt} & .............................................\\
podpis promotora & \hspace*{100pt} & podpis autora
\end{tabular}
\end{center}
%--------------------- 02. ABSTRAKTY -----------------------------------------------------------------------
%-----------------------------------------------------------------------------------------------------------


{
\begin{abstract}

\begin{center}
\tytul
\end{center}
Praca omawia wybrane zagadnienia z obszaru procesowania języka naturalnego skupiając się na możliwociach ich wykorzystania dla języka polskiego. W pracy umieszczono wyniki i analizę rezultatów zastosowania tych metod na przykładowym zbiorze danych tekstowych. \\
Zbiór danych wybrany do analiz to transkrypcje przemówień sejmowych z lat 1989 - 2019 r. \\
Praca skupia się na analizie i zastosowaniu następujących tematów: 
\begin{itemize}
\item modele n-gramowe,
\item modelowanie tematyczne,
\item analiza sentymentu.
\end{itemize}
Wykonano również analizy łączące tematy ze sobą oraz modele wykorzystujące wszystkie przygotowane informacje o korpusie.
\end{abstract}
}


%--------------------- 03. OSWIADCZENIE --------------------------------------------------------------------
%-----------------------------------------------------------------------------------------------------------


\null \hfill Warszawa, dnia ..................\\

\par\vspace{5cm}

\begin{center}
Oświadczenie
\end{center}

\indent Oświadczam, że pracę pod
tytułem ,,\tytul '', której promotorem jest \mbox{\supervisor}, wykonałam
samodzielnie, co poświadczam własnoręcznym podpisem.
\vspace{2cm}


\begin{flushright}
  \begin{minipage}{50mm}
    \begin{center}
      ..............................................

    \end{center}
  \end{minipage}
\end{flushright}

\thispagestyle{empty}
\newpage



%--------------------- 04. SPIS TRESCI ---------------------------------------------------------------------
%-----------------------------------------------------------------------------------------------------------
\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}


%--------------------- 05. ZASADNICZA CZESC PRACY ----------------------------------------------------------
%-----------------------------------------------------------------------------------------------------------
\null\thispagestyle{empty}\newpage
\pagestyle{fancy}
\pagenumbering{arabic}
% -> NOTE: JEŻELI Z POWODU DUŻEJ ILOŚCI STRON W SPISIE TREŚCI SIĘ NIE ZGADZA, TRZEBA ZMODYFIKOWAĆ RĘCZNIE
%\setcounter{page}{11} 

%\chapter*{Wstęp}
%\markboth{}{Wstęp}
%\addcontentsline{toc}{chapter}{Wstęp}
\chapter{Wstęp}
%\markboth{}{Wstęp}
%\addcontentsline{toc}{chapter}{Wstęp}
W ostatnich latach obszar przetwarzania języka naturalnego (z ang. \textit{NLP - natural language processing}) rozwija się bardzo intensywnie. Powstają nowe rozwiązania pozwalające na zaawansowane przetwarzanie i analizę nieustrukturyzowanych danych jakimi są dane tekstowe. Coraz bardziej zaawansowane są modele pozwalające na interpretację jak również generowanie tekstu. \\
Do niedawna możliwoci wykorzystania tych narzędzi w pracy z językiem polskim były ograniczone, lecz obecnie dostępnych jest już wiele pakietów oraz narzędzi, które znacznie ułatwiają zadania tego typu.

\section{Cel i zakres pracy}\label{miss_vs_class}

Celem niniejszej pracy jest przegląd narzędzi NLP dostępnych dla języka polskiego oraz analiza zastosowań niektórych z nich na przykładowym zbiorze danych. \\
Niniejsza praca skupia się na takich elementach NLP, które przy względnie niskim poziomie złożoności mogą przynieść interesujące rezultaty. Badano możliwości w zakresie prostego generowania tekstu, określania tematu wypowiedzi i jej sentymentu. Starano się połączyć te elementy podczas kontrukcji modeli, które na podstawie pewnych charakterystyk tekstu, potrafiłyby odgadnąć jego autora i określić jego cechy. 
Dane, na których przeprowadzono analizy zawierają transkrypcje wystąpień sejmowych z lat 1989 - 2019 r. Dołączono do nich podstawowe informacje o autorze, co pozwoliło rozszerzyć zakres analiz o budowę modeli predykcyjnych różnego typu.

\section{Przegląd rozdziałów}
W pierwszym rozdziale pracy zaprezentowano przegląd pojęć i zgadnień związanych z przetwarzaniem języka naturalnego, które pojawiają się w niniejszej pracy. \\

W drugim rozdziale pracy zaprezentowano krótki przegląd narzędzi, które mogą zostać wykorzystane podczas pracy z analizami języka polskiego. pakiety , embeddingi, morfeusz \\

W trzecim rozdziale opisano zbiór danych, na którym przeprowadzano kolejne analizy. Znajduje się tu opis przetworzeń, jakie były konieczne do poszczególnych zadań. Każde zagadnienie wymagało innego podejścia do korpusu i innego zestawu informacji. Przeprowadzono dwa etapy czyszczenia danych, lematyzację, wektoryzację oraz przypisanie sentymentu. \\

W rozdziale czwartym opisano rezultaty otrzymane podczas pracy z modelami n-gramowymi. Na potrzeby tych analiz napisano w języku python klasę dedykowaną modelom tego typu. Zaimplementowane metody pozwalały m.in. na 
\begin{itemize}
\item konstrukcję modeli o różnej złożoności i dla różnych wymiarów,
\item generowanie tesktu w opartciu o dany obiekt typu model,
\item kalkulację miary perplexity dla danych fragmentów tekstu
\item generowanie zdania-reprezentanta danego modelu.
\end{itemize}


%---------------------------Rozdzial 2
\chapter{Podstawowe elementy i zagadnienia związane z procesowaniem języka naturalnego}
Procesowanie języka naturalnego jest bardzo obszerną dziedziną data science. Pokrywa szeroką gamę zagadnień rozpoczynając od najprostszych typu przewidywanie kolejnego słowa przy pisaniu SMSów na telefonie, po interpretację tekstu oraz tłumaczenie maszynowe. To właśnie zagadnienia związane z tłumaczeniem były tematem pierwszych projektów w obszarze NLP już w latach pięćdziesiątych XX wieku. \\
Podstawowe elementy przygotowywania danych do analiz związanych z przetwarzaniem języka naturalnego to:

\begin{itemize}
 \item tokenizacja, czyli podział tekstu na segmenty, najczęściej pojedyncze słowa,
 \item stemming ma na celu obcięcie wszystkich przyrostków i przedrostów aby zbliżyć słowo do podstawowej postaci,
 \item lematyzacja to przypisanie do każdego słowa jego formy podstawowej, która go reprezentuje, 
 \item tworzenie wektorów własnościowych (word embeddings) w uproszczeniu będących wektorową reprezentacją znaczenia danego słowa.
\end{itemize}


%---------------------------Rozdzial 3
\chapter{Dane i środowiska wykorzystane do analiz}

\section{Źródło danych}
Zbiór na którym zostaną przeprowadzone analizy został pobrany z serwisu kaggle.com (\url{https://www.kaggle.com/paweladamczak/polish-politicians-speeches}). Zawiera transkrypcje przemówień polskich polityków  z lat 1989 – 2019 oraz profil każdego z mówców. \\
Zbiór powstał poprzez zaczytanie plików w formacie pdf, które można znaleźć w archiwum sejmowym pod adresem \url{http://www.sejm.gov.pl/Sejm9.nsf/page.xsp/archiwum}. Większość plików ma charakterystyczny układ, w którym każde wystąpienie podzielone jest na części opatrzone nagłówkiem wskazującego autora tekstu umieszczonego poniżej. Niestety nie dla wszystkich kadencji został zachowany ten układ, co przełożyło się na problemy z identyfikacją autorów dla części wystąpień. Zostało to opisane w rozdziale \ref{section:autor}.

\section{Format danych}
Dane zostały udostępnione w formie bazy danych SQL zawierającej dwie tabele. Pierwsza z nich zawiera szczegółowy profil każdego polityka. W tabeli występuje 2626 posłów oraz 68 różnych wskazań partii z 8 kadencji. Imię i nazwisko posła nie jest kluczem głównym, ponieważ mogą się one pojawiać wielokrotnie w różnych kadencja i z przypisaniem do różnych partii. Najistotniejsze informacje zawarte w tej tabeli przedstawiono w Tabeli \ref{t1}. \\

Druga tabela zawiera transkrypcje przemówień sejmowych. Najistotniejsze kolumny przedstawiono w Tabeli \ref{t2}. 
Tabela zawiera 272 321 wierszy, w tym 272 221 z niepustym polem \textit{speech\_raw}. Niektóre wiersze zawierają fragmenty tych samych wystąpień. Dzieje się tak w wypadku, gdy przemówienie było przerywane wypowiedziami innych mówców. Unikalnych wystąpień znajduje się z bazie ok. 19 tys. \\
Ponieważ dane były dostarczone w formie bazy SQL, część analiz wykonano w tym środowisku. Większość analiz na dalszym etapie wykonano w języku python.


\begin{table} \centering
\begin{tabular}{ |c|c| } 
 \hline
 Nazwa kolumny w zbiorze & Opis  \\ \hline
 full\_name & imię i nazwisko \\  \hline
 elected & data wybrania na posła \\ \hline
graduated\_school & ukończona szkoła/uczelnia \\ \hline
education\_level & wykształcenie \\ \hline
occupation & zawód \\ \hline
party\_section & partia \\ \hline
number\_of\_votes & liczba otrzymanych głosów \\ \hline
languages & znane języki \\ \hline
last\_party & ostatnia partia \\ \hline
\end{tabular}
\caption{Dane dotyczące autorów wystąpień}
\label{t1}
\end{table}

\begin{table}[h] \centering
\begin{tabular}{ |c|c| } 
 \hline
 Nazwa kolumny w zbiorze & Opis  \\ \hline
session\_number & numer sesji \\ \hline
date\_ & data wystąpienia \\ \hline
number\_ & numer porządku obrad  \\ \hline
speech\_title & tytuł wystąpienia \\ \hline
speech\_raw & tekst wystąpienia \\ \hline
\end{tabular}
\caption{Dane dotyczące wystąpień} \label{t2}
\end{table}


\section{Algorytm przypisania autora tekstu}
\label{section:autor}

Podczas przeglądania danych zidentyfikowano błędy w przypisaniu id autorów przemówień. Konsekwencją tego jest brak możliwości łatwego przypisania autora danego wystąpienia do jego tekstu. Błędy występowały dla większości wpisów w bazie i mimo tego, że sprawiały wrażenie systematycznej zmiany (poprawna wartość wydawała się bliska z kontekście porządku leksykograficznego), nie udało się ustalić algorytmu, który mógłby przypisać poprawne wartości w sposób automatyczny.  \\
W związku z powyższym przypisanie autora (\textbf{author\_final}) wykonano w następujących krokach (10\_ author\_final.sql):
\begin{enumerate}
\item Wiele wystąpień zaczyna się od schematu „Poseł Imię Nazwisko:”. Pierwsze występujące w takim kontekście imię i nazwisko przypisano jako dane autora z tekstu (kolumna author\_by\_text).
\item Przypisano imię i nazwisko autora w oparciu o id\_ z bazy (kolumna author\_by\_id).
\item Dla przypadków gdzie dla danego id, chociaż raz author\_by\_text jest równe author\_by\_id, przypisano te wartości jako author\_final.
\item Dla pozostałych przypadków sprawdzono jaka wartość author\_by\_text pojawia się najczęściej w obrębie danego id. Jeśli wartość była niepusta, została przypisana jako author\_final.
\item Pozostałe przypadki to takie, gdzie najczęściej pole author\_by\_text było puste, tj. w tekście wystąpienia nie występowało zestawienie „Poseł Imię Nazwisko:”. Wynika to z różnej postaci plików  transkrypcją – dla części z nich fragmenty pogrubione na poniższym obrazku nie znalazły się w tekście dostępnym w bazie.
\begin{figure}[h]
\includegraphics{screen_pdf}
\centering
\end{figure}
\item W trakcie analizy danych niektóre id były weryfikowanych ręcznie w oryginalnych plikach pdf. Wartości dla nich wprowadzono ręcznie w kodzie.
\end{enumerate}
Po zastosowaniu powyższych kroków w bazie brakowało przypisania author\_final dla 795 id (na 3828). Dla pojedynczych fragmentów wystąpień uzupełnienie wyniosło ok 83\%.


\section{Czyszczenie danych}
Wykonano następujące etapy czyszczenia danych: 
\begin{itemize}
 \item usunięto znaki specjalne 
 \item wykasowano tytuły przemówień pojawiające się na początku każdego fragmentu
 \item usunięty fragmenty tekstu w nawiasach (np. (Oklaski.), (Dzwonek.))
 \item Usunięto fragmenty identyfikujące mówcę, tj. fragment „Poseł Imię Nazwisko:”
\end{itemize}

Następnie przygotowano drugi zestaw tekstów, które oczyszczono jeszcze bardziej z fragmentów mało informacyjnych takich jak:
\begin{itemize}
 \item "Poseł Imię Nazwisko:"
 \item Panie Marszałku!
 \item Pani Marszałek!
 \item Wysoka Izbo!
 \item Panie Ministrze!
 \item Dziękuję bardzo.

\end{itemize}

Po tym etapie czyszczenia w bazie pozostało 272 217 fragmentów przemówień o niezerowej długości, z których 225 385 ma przypisanego autora. Oba zestawy tekstów z różnym poziomem oczyszczenia będą stosowane w różnych analizach. Przykładowo do konsttrukcji modeli n-gramowych wykorzystanoo oba zestawy danych. \\
Fragmenty tych samych wypowiedzi, zgrupowane po dacie, autorze i tytule zostały połączone.
Po usunięciu tekstów, które po wszystkich modyfikacjach stały się puste, w bazie zostało \textbf{158 885} tekstów z przypisanym autorem.


%--------------------------- rozdzial 4
\chapter{Wstępna analiza danych}

Dla tekstów z przypisanym autorem przeprowadzono wstępną analizę danych. Po usunięciu cyfr, znaków interpunkcyjnych oraz słów z listy polskich \textit{stopwords} analizowano częstości w celu weryfikacji i rozszerzenia listy \textit{stopwords}. W wyniku analizy dodanie do niej następujące elementy, ze względu na niską wartość informacyjną i ryzyko wprowadzenia zaburzeń do dalszych analiz:
\begin{itemize}
\item pkt \item art \item 'Wysoka Izbo!' \item 'Panie Ministrze!' \item 'Dziękuję bardzo.'
\end{itemize}

Po wykonaniu tego czyszczenia, w korpusie znajduje się nieco ponad 42 mln słów. Przed lematyzacją w korpusie występuje 367 716 różnych słów.\\
Najczęściej wypowiadający się posłowie znajdują się w Tabeli \ref{t41}. 

\begin{table}[h] \centering
\begin{tabular}{|c|c|c|}
\hline
author\_final & Liczba slów	& Liczba wypowiedzi \\\hline
Stanisław Stec	& 477862 & 	1368 \\\hline
Mirosław Pawlak	& 203379 & 	1134 \\\hline
Romuald Ajchler	& 401879 	& 1120 \\\hline
Jan Kulas	& 547001	& 1101 \\\hline
Andrzej Szlachta	& 242306	 &  982 \\
\hline
\end{tabular} \caption{Najczęściej wypowiadający się posłowie} \label{t41}
\end{table}

Posłowie z największą liczbą słów w wypowiedziach znajdują się w Tabeli \ref{t42}.\\\\\\\\


\begin{table}[h!] \centering
\begin{tabular}{|c|c|c|}
\hline
author\_final	& Liczba slów	& Liczba wypowiedzi \\\hline
Jan Kulas	& 	547001	& 	1101 \\\hline
Józef Zych	& 	514652	& 	878 \\\hline
Jerzy Jaskiernia	& 	481512	& 	658 \\\hline
Stanisław Stec	& 	477862	& 	1368 \\\hline
Tadeusz Tomaszewski	& 	412781	& 	917 \\\hline
\end{tabular} \caption{Posłowie z najwięszą liczbą słów} \label{t42}
\end{table}

Słów, które można określić jako „rzadkie”, tj. występujące w całym korpusie nie więcej niż 5 razy jest 226 599 (w tym 126382 występuje tylko jeden raz), czyli stanowią one istotną większość. Przykładowe słowa pojawiające się dokładnie jeden raz w całym korpusie:\\
 zaanektować, babette, opowiadaną, odejmowana, trawiona, kolektywistyczna, zagonów, troić, maćkowy, centralizowany.\\
Słowa pojawiające się najczęściej, to:
\begin{table}[h] \centering
\begin{tabular}{| c | c |}
\hline
słowo & liczba wystąpień \\ \hline
rząd & 95603 \\
projektu & 95657 \\
prawa &  96092 \\
chodzi & 99480 \\
panie & 108932 \\
projekt & 113847 \\
pytanie & 116217 \\
pracy & 118301 \\
komisji & 146153 \\
państwa & 148339\\
\hline
\end{tabular}
\caption{Słowa pojawiające się najczęściej w korpusie}
\end{table}

%------------------------------------------------ rozdział 5

\chapter{Modele n-gramowe}

Pierwszym elementem analizowanym na danym korpusie były modele n-gramowe. Takie modele bazują na statystykach występowania n-gramów w analizowanym korpusie i mogą służyć do przewidywania kolejnego elementu sekwencji, jak również do generowania nowych sekwencji w~oparciu o zaobserwowane zależności. Można budować zarówno modele oparte o słowa jak i inne elementy jak pojedyncze znaki czy sylaby. Sekwencje 1-gramowe nazywane są unigramami, 2-gramowe bigramami a 3-gramowe trigramami. Wraz ze wzrostem parametru \textit{n} wzrasta zdolność modelu do odzwierciedlania bardziej złożonych zależności pomiędzy słowami, ale istotnie rośnie zapotrzebowanie na dane do jego konstrukcji.
Przykładowo dla modelu opartego o słowa prawdopodobieństwo, że w naszej sekwencji, po słowie „praca” wystąpi słowo „zaliczeniowa” wynosi:
\begin{equation}
P(zaliczeniowa|praca) = \frac{cnt(praca\_ zaliczeniowa)}{cnt(praca \_\_\_)}
\end{equation}
Gdzie \_\_\_ oznaca dowolne słowo a liczniki „cnt” zostały wygenerowane na korpusie, na którym trenowano model.
W praktyce, aby uzyskać rozkłąd prawdowpodobieństwa, należy zastosować wygładzanie, które zaadresuje problem zwracania prawdopodobieństw dla słów, które nie znalazły się w oryginalnym korpusie.\\
Modele n-gramowe są wykorzystywane m.in. w rozpoznawaniu mowy, badania poprawności pisowni i tłumaczeniu maszynowym.\\
W niniejszej pracy skupimy się na:
\begin{itemize}
\item generowaniu treści,
\item szacowaniu prawdopodobieństwa pojawienia się okrs=eślonego zdania w wypowiedzi,
\item znajdowaniu \textit{zdania - reprezentanta} dla danego korpusu.
\end{itemize}

Modele powstawały w wersjch wersjach dla 1, 2 oraz 3-gramów. Dla obu opcji w pierwszym kroku przygotowano słownik zawierający wszystkie występujące kombinacje odpowiednich n-gramów ze słowami po nich następującymi. 
Aby uprościć i zautomatyzować konstrukcję modeli, przygotowano klasę ng\_models (ng\_models.py). Obiekt tej klasy jest modelem n-gramowym utworzonym zgodnie z przekazanymi parametrami n oraz corpus. Tworzenie modelu odbywa się z uwzględnieniem znaków specjalnych oznaczających początek i koniec zdania. Podczas jego konstrukcji następuje przetworzenie korpusu metodą \textit{process\_text}, która m.in. usuwa znaki specjalne, duże litery, wielokrotne spacje.

\section{Generowanie przemówień} \label{section:gen}

Generowanie przemówień polega na przypisaniu słowa początkowego (lub wybraniu losowego) a następnie w sposób wybieraniu słów kolejnych na bazie słownika prawdopodobieństw zbudowanego w poprzednim punkcie.
Wraz ze wzrostem n, rośnie zapotrzebowanie na dane do modelu. W przypadku gdy danych do uczenia jest zbyt mało, model będzie po prostu odtwarzał zdania, które pojawiały się w danych uczących. Z~drugiej strony takie modele potrafią generować sekwencje lepszej jakości, bardziej przypominające prawdziwe zdania, ponieważ zachowują więcej logicznych powiązań pomiędzy kolejnymi słowami.
Do zbudowania najprostszych modeli nie były wykorzystane żadne z pakietów związanych z NLP. 

Za generowanie wypowiedzi odpowiada metoda \textit{generate} klasy \textit{ng\_models} pozwalająca generować sekwencje zdań zadanej długości. Metoda zaczyna od losowego słowa z listy słów pojawiających się na początku zdań. Każde zdanie generowane jest oddzielnie do momentu aż zostanie wylosowany znacznik końca zdania, lub zostanie przekroczony parametr ograniczający długość zdania.\\
Przykładowe przemówienie dla modelu 1-gramowego wygenerowane z parametrami (steps=10, max\_sent=10):\\
\textit{Panie marszałku. Poza tym pracownikom projekt ordynacji podatkowej dla polski 1. Nie przekreślamy sprawę jasno sformułowane są niezgodne z dwiema. Jak zostanie zwrócony jako szef zespołu na wykreśleniu pkt. Boję się o swoich list to zasłużone dla strażaków. Chciałbym skupić wyłącznie do spożycia alkoholu oraz refundacji. Wysoka izbo. Jakie gwarancje zastawnika niż pobierających świadczenia z góry rozdzielanie. Swoich dzieci. W praktyce zostało wyartykułowane.} \\

Przykładowe przemówienie dla modelu 1-gramowego wygenerowane z parametrami (steps=10, max\_sent=20):\\
\textit{Natomiast takich uchwał dotyczących kwestii warunków prawnych w budowie. Szanowny panie marszałku. Pierwsza. W rozporządzeniu rady sądownictwa dopiero na swoich kompetencji i. To kolejny projekt i szczególnej państwowej służby ochrony państwa. W takiej potrzeby zwiększania środków europejskich demokratów formułuje wnioski. Szanowni państwo dzisiaj o szczególne znaczenie specjalnych jest wierną. Powiedzieliśmy do spółki na stworzenie nowej rezerwy zostanie skazany. Panie marszałku. Manipulowanie tą wersją projektu skłania do kilkuletnich zaniedbań i. Pierwszy pracę. Dopiero w związku z przeznaczeniem a przede wszystkim informacyjne. To proszę państwa dla poszczególnych powiatów oraz zniesienie ograniczeń. Przypuszczam że każda ze strony młodzieży oraz upoważnienie na. Później próbuje ogarnąć tej sprawie wyjścia naprzeciw wspomnę np. Można było. Potrzeba uchwalenia zaproponowanego przez radę ministrów. Jej do pana ministra tchórzewskiego byłego likwidatora majątku zagrożonego. Wprowadza tę izbę nie udało mi wiadomo że albo. Wysoki sejmie.}\\

Łatwo jest ocenić, że powyższe wypowiedzi nie pochodzą z prawdziwych wystąpień sejmowych. Możliwych kombinacji słów następujących po sobie jest zbyt dużo aby taki model mógł zachować logikę pomiędzy kolejnymi elementami zdania. Modele bigramowe powinny lepiej sobie z tym radzić.\\\\
Przykładowe przemówienie dla modelu 2-gramowego wygenerowane z parametrami (steps=10, max\_sent=10):\\
\textit{Mam kłopot z budowaniem nowych mieszkań w złym świetle respektowanie. To podchodzi pod administrację. Chodzi zapewne o wiele trudniejsza jest sytuacja w państwie członkowskim. Jeżeli obie strony uznały za obowiązujące zajmowanie przez 10 lat. Panowie osiągnęli a my słyszymy sto takich zgromadzeń. Jeżeli uzyskaliśmy technologie na warunkach partnerskich; wtedy jest ona nam. Do orzekania... Tyle jesteś samorządny o tyle obecnie średnio 133 osoby na. Przyjęcie kompromisowych rozwiązań łączących projekt rządowy nad którym dyskutujemy to. Jak kupuję jakiś towar.}\\

Przykładowe przemówienie dla modelu 2-gramowego wygenerowane z parametrami (steps=10, max\_sent=20):\\ \textit{Chciałbym wykreślić ze sprawozdania... Powoli zanika a mogła to być może wykorzystawszy także niezły poziom techniczno-technologiczny jest znacznie lepszy niż ten który złożył przysięgę. Jednak prenumerować przez pocztę przez swoich i dlatego nie ma powodu do jakichkolwiek sytuacji które mogą naruszać wolności wykonywania zawodu. Czy zrewaloryzowana ale też o rozruchu. Nie ustosunkował.właśnie do tego że komitet badań naukowych lub organizacji społecznej o czym mówił pan prezes socha był łaskaw pan. To przekształcane w pracowniczy program emerytalny jest instytucją samofinansującą się która jest w pełni popiera i w imieniu posłów z. Będziemy kontrolować waszą działalność trybunał stanu który decyzją sejmu termin ten wydaje się tu toczy będzie miała zastosowanie w postępowaniu. Nie ochronę uprawnień najemców i dzierżawców które już dzisiaj plagi społecznej. Mówię wewnętrzna organizacja tego ratownictwa także. Gdyby obecne napięcia między przedstawicielami ministerstwa skarbu czy po przyjęciu będzie miała jedno pytanie: co pani poruszyła są prawnie chronione.}\\

Widać, że w drugim modelu tekst bardziej przypomina realnie wystąpienia i prawdziwe zdania. Oczywiście w dalszym ciągu nie ma żadnych wątpliwości, że nie są to realne wypowiedzi, ale ich czytanie jest nieco przyjemniejsze niż dla poprzedniego modelu.
Podjęto również próbę zbudowania modelu 3-gramowego. Czas kalkulacji wydłużył się istotnie. Budowa słownika do modelu trwała prawie 30 minut.\\
Przykładowy tekst wygenerowany z parametrami (steps=10, max\_sent=10):\\ \textit{Czy elementem realizacji polityki gospodarczej. Już zadała to pytanie bo jest tajemnicą poliszynela unikają one płacenia miliardowych podatków i nadmiernie wykorzystują pracowników. I~marek belka nie budzi naszego zaufania. Dziękuję unii wolności za to że ma wygrać najlepszy. Chciałabym przytaczać w tej chwili problem bezrobocia mimo że ten dokument został odtajniony decyzją wiceministra spraw wewnętrznych i administracji i innym. Wiadomo są jedne z głównych czynników utrudniających udany proces integracji repatriantów z polskim społeczeństwem z obywatelami żeby przez zaniechania rządu nie. Przecież trudno się spodziewać tego że przy wyważeniu argumentów ograniczenie czasu zgłaszania weta też ma pewne koncepcje które należałoby wysokiej izbie. Połowa to mogą być grupy członkowskie. Wtedy ta nowelizacja rzeczywiście mogła wejść w życie na drugim etapie jego nowelizacji. Panie prokuratorze zwrócić uwagę że istnieje bardzo złe mniemanie o politykach i o polityce wobec cudzoziemców polityce imigracyjnej jest takim dzwonem.}\\

Zdania sprawiają wrażenie lepiej skonstruowanych niż dla poprzednich modeli. Jednak stosunkowo mały zbiór danych uczących sprawia, że model w dużym stopniu powtarza całe zdania ze zbioru uczącego.
Przyjrzyjmy się jak wygląda słownik dla niektórych 3-gramów występujący w powyższym tekście. \\

\includegraphics[scale=0.8]{s1}

\begin{figure}
\includegraphics[scale=0.8]{s2}
\end{figure}
\begin{figure}
\includegraphics[scale=0.8]{s3}\\
\includegraphics[scale=0.8]{s4}
\end{figure}


Z powyższego wynika, że do budowy dobrego modelu dla większych wartości \textit{n} dla języka polskiego potrzebne są dużo większe zestawy danych. Dla języków z prostszymi gramatykami, gdzie nie występuje tak wiele odmian słów, wymagania te mogą być niższe.


\section{Badanie prawdopodobieństwa wystąpienia danego zdania} \label{section:perp}
Dla modeli n-gramowych mamy możliwość przeanalizowania spójności danej sekwencji z modelem. Tj. jak prawdopodobne jest, że dane zdanie pochodzi z danego modelu. Można to szacować za pomocą wartości \textit{perplexity}, która generalnie służy do oceny jakości modelu n-gramowego na danych testowych. Dla pojedynczego zdania o licznie wyrazów \textit{N} możemy wyliczyć:

\begin{equation}
Perplexity=p(w_{test})^{-\frac{1}{N}}
\end{equation}
\begin{equation}
p(w_{test})=\prod_{i=1}^{N+1} p(w_{i} | w_{i-n+1}^{i-1})
\label{eq53} \end{equation}

Liczniki i indeksowanie są tak dopasowane aby uwzględniać pozycję początkową i końcową wyrazu w zdaniu poprzez uwzględnianiu w analizach dodatkowych tokenów oznaczających początek i koniec zdania.
Aby zaadresować kwestię zerowania się iloczynu w przypadku pojawienia się n-gramów spoza korpusu, na którym trenowano model, wprowadza się różne metody wygładzania tej miary. Tutaj zastosujemy „add-1 smoothing”, która modyfikuje kalkulację pojedynczych prawdopodobieństw w następujący sposób:

\begin{equation}
p(w_{i} | w_{i-n+1}^{i-1}) = \frac{c(w_{i-n+1}^i)+1}{c(w_{i-n+1}^{i-1}) +V}
\label{eq54} \end{equation}
Aby zapewnić, że otrzymane wartości będą poprawnym rozkładem prawdopodobieństwa, wartość \textit{V} musi być równa liczbie możliwych kontynuacji każdej sekwencji powiększonej o 1 ze względu na możliwy koniec zdania.\\

Za kalkulację tej wielkości odpowiada metoda \textit{perplexity} zaimplementowana dla klasy \textit{ng\_models}. Im mniejsza wartość perplexity, tym większa spójność zdania testowego z korpusem, na którym uczony był model.
Przykładowo dla modelu 2-gramowego z poprzedniego punktu otrzymujemy dla zdania \textit{"Dzisiejsze posiedzenie ma na celu omówienie projektu ustawy"}’ wartość 109 921 a dla zdania \textit{"Lwy to zwierzęta kotowate żyjące na sawannie"} 4 240 107.

\section{Jakość i porównywalność modeli budowanych na mniejszych korpusach}
Jak można zaobserować na wynikach otrzymanych w rozdziale \ref{section:gen}, jakość modelu rozumiana jakość zdolność do imitacji tekstu z korpusu poprawia się istotnie przy wzroście parametru \textit{n}. Wymaga to jednak korpusu o dużym rozmiarze. W tym rozdziale spróbujemy odpowiedzieć na pytanie, czy mniejsze korpusy pozwalają na budowę modeli o interesujących właściwościach. Skupimy się na określaniu i mierzeniu różnicy pomiędzy modelami, z wykorzystaniem miary wprowadzonej w rozdziale \ref{section:perp}.

\subsection{Modele dla pojedynczych mówców}
Ze względu na niedużą wielkość korpusu po podzieleniu na poszczególnych autorów, na potrzeby tej analizy konstruowano wyłącznie modele unigramowe. Zbudowano oddzielne modele dla 5 polityków z największą liczbą słów w danych (400 – 500 tys.) oraz dla trzech z istotnie mniejszą ilością słów (<100 tys.). Porównano wyniki dla kilku mniej lub bardziej prawdopodobnych zdań. Wyniki znajdują się na rysunku \ref{pic:51}. \\

Na szaro zaznaczone są osoby, ze stosunkowo dużym zbiorem danych uczących. Widać, że ma to istotny wpływ w sytuacjach gdy oceniane są zdania „mało prawdopodobne”. Wówczas w kalkulacji prawdopodobieństwa ciągu wyrazów pojawiają się bardzo małe wartości, co z kolei przekłada się na wysokie perplexity. Ogółem prawdopodobieństwa mają podobne trendy dla wszystkich autorów, ale warto zauważyć zaburzenie kolejności pomiędzy zdaniami „Demokracja jest najlepszym systemem politycznym” a „Zupełnie przypadkowy układ pięciu słów”. Niemniej cieszy, że sekwencja „lew czarownica i stara szafa” w każdym przypadku są opcją najmniej prawdopodobną.\\\\\\

\begin{figure}[h] 
\includegraphics[scale=1.3]{p1} 
\centering
\caption{Miara perplexity dla modeli budowanych per mówca} \label{pic:51}
\end{figure} 
Wykres \ref{pic:52} pokazuje rozrzut wartości zależny od modelu, który pokazuje, że przy tak małych zbiorach uczących, trudno jest porównywać wartości pomiędzy modelami.\\

\begin{figure}[h] 
\includegraphics[scale=0.8]{p2} 
\centering
\caption{Wizualizacja miary perplexity dla modeli budowanych per mówca} \label{pic:52}
\end{figure}
Niemniej przeglądając słowniki dla poszczególnych modeli budowanych w ten sposób, widać, że cechuje je duża przypadkowość. Zbyt wiele n-gramów nie będzie się w nich odnajdywać i będą kontrybuowały do miary z tą samą, najniższą wartością. Zdecydowanie też miara ta nie nadaje się do porównywania  modeli budowanych na korpusach o istotnie różnej liczności.

\subsection{Modele dla partii}
Ponieważ modele budowane dla pojedynczych osób wydawały się budowane na zbyt małych korpusach, powtórzono analizy grupując autorów wg najnowszego przypisania do partii (SQL). Liczności słów w korpusie dla głównych partii wyglądają następująco:
\begin{center}
\begin{tabular}{|c|c|c|} 
\hline
partia & liczba słów & liczba wypowiedzi \\ \hline
Prawo i Sprawiedliwość & 12 409 825 & 31 630 \\ \hline
Sojusz Lewicy Demokratycznej & 13 143 036 & 27 129 \\ \hline
Platforma Obywatelska & 9 438 332 & 24 977 \\ \hline
Polskie Stronnictwo Ludowe & 8 256 113 & 17 851 \\ \hline
posłowie niezrzeszeni & 3 735 227 & 7 358 \\ \hline
Akcja Wyborcza Solidarność & 3 304 864 & 5 642 \\ \hline
Samoobrona & 2 027 951 & 4 407 \\ \hline
\end{tabular}
\end{center}
Wartości są istotnie wyższe, oczekiwane są więc stabilniejsze wyniki. \\
Zweryfikowano wartości perplexity na tym samym zbiorze zdań testowych. Wyniki pokazano na rysunku \ref{pic:53}. \\\\
\begin{figure}[h] 
\includegraphics[scale=0.6]{p3} 
\centering
\caption{Miara perplexity dla modeli budowanych per partia} \label{pic:53}
\end{figure} 
\begin{figure}[h] 
\includegraphics[scale=0.6]{p4} 
\centering
\caption{Wizualizacja miary perplexity dla modeli per partia} \label{pic:54}
\end{figure}

Na wykresie \ref{pic:54} porównano wyniki dla modeli unigramowych i bigramowych.
Można zauważyć, że dla modelu 1-gramowego wartości perplexity są dużo niższe co wynika m.in. z tego, że łatwiej trafić na wyrażenie dwuwyrazowe, które się w modelu pojawia niż 3 wyrazowe. Na pewno widać większą stabilność wyników. Porównywanie konkretnych wartości pomiędzy modelami zdecydowanie ma więcej sensu.



\section{Budowa zdań w oparciu o maksymalizację prawdopodonieństwa}
Poniższe analizy wykonano na tekście oczyszczonym z najczęściej pojawiających się, mało informacyjnych fraz typu „Panie Marszałku!”. \\
Znając prawdopodobieństwa następowania kolejnych słów po sobie, można spróbować zbudować zdanie minimalizujące wartość perplexity, tj. maksymalizujące prawdopodobieństwo „zobaczenia go w modelu”. W tym celu wyposażono klasę \textit{ng\_models} o metodę \textit{mprob\_sent}. Celem tego ćwiczenia było zbadanie możliwości otrzymania zdania, które byłoby najbardziej charakterystyczne dla danego korpusu, czyli można by je określić jako dewizę związaną z danym tematem, osobą czy też grupą. \\
Metoda zakłada pobranie pierwszego słowa zdania od użytkownika a następnie wybieraniu kolejnych słów wg prawdopodobieństw w modelu. Czyli jeśli słowem początkowym jest słowo „poseł”, metoda sprawdzi jakie n-gramy pojawiały się w modelu zaczynające się od słowa poseł i wybierze najczęstszy. Przykładowe najczęstsze następniki słowa „poseł” w modelu 1-gramowym dla wybranego mówcy.\\

\begin{figure}[h] 
\includegraphics[scale=0.8]{m1} 
\centering
\end{figure}
 
Niestety okazuje się, że taki algorytm bardzo łatwo wpada w pętle. Przykładowe zdania otrzymane w wyniku jego działania to:\\
\textit{Poseł sprawozdawca mówił o tym że w tym że w tym że w tym że w tym że w tym że w tym że w tym że w tym że w tym.} \\
\textit{Ja to jest to jest to jest to jest to jest to jest to jest to jest to jest to jest to jest to jest to jest to jest to jest to.}\\
Powód jest zobrazowany na poniższym obrazku.
\begin{figure}[h] 
\includegraphics[scale=0.8]{m2} 
\centering
\end{figure}

W takim przypadku, jeśli model trafi w pewnym momencie na słowo „to”, zapętli się i będzie powtarzał 2-gram „to jest”. \\
Jednym z możliwych rozwiązań jest zakaz powtarzania słów w obrębie jednego zdania. W przypadku trafienia na słowo, które już się pojawiło w trakcie generowania, wybieramy kolejne wg częstości pojawiania się. Kończymy generowanie, gdy sytuacja powtarza się zbyt często. Wówczas otrzymujemy np.:\\
\textit{Poseł sprawozdawca mówił o tym że w tej ustawy.}\\
\textit{Ja to jest bardzo ważne.}

W przypadku, gdy do metody nie zostanie przekazane pierwsze słowo, generowanie rozpocznie się od słowa, które najczęściej pojawia się jako pierwsze w zdaniach w danym korpusie. W poniższej tabeli umieszczono porównanie wyników dla różnych osób w zależności od słowa początkowego.\\

\begin{figure}[h] 
\includegraphics[scale=1.1]{m3} 
\centering \caption{Zdania wygenerowane w oparciu o maksymalizację prawdopodobieństwa - politycy}
\end{figure}

Niestety wyniki są do siebie bardzo podobne. Wygląda na to, że mimo rozróżnienia na poszczególnych mówców, w modelach zostały odzwierciedlone zasady ogólne języka. 
Długość otrzymanych zdań jest bezpośrednią pochodną skłonności do zapętlania się słów w danym korpusie. Widać, że dla większości najczęstszym słowem jest „w”.\\
Jako alternatywną metodę, która mogłaby wpłynąć na „urozmaicenie” generowanych zdać, rozważano usunięcie z korpusu tzw. \textit{stopwords}. To jednak mogłoby generować zdania nienaturalne w swojej gramatyce oraz zupełnie już pozbawione logiki nawet w horyzoncie kilku słów. Testowano również wykorzystanie modeli bardziej złożonych, tj. trigramowych, niestety te najczęściej cytowały dosłownie jedno ze zdań korpusu.\\
Również generowanie najbardziej prawdopodobnego zdania z modeli dla partii przyniosło rozczarowujące rezultaty. Okazuje się, że dla tego zadania zwiększony rozmiar korpusu powoduje, że modele 1-gramowe stają się zbyt podobne. Generowane zdania były prawie identyczne. \\

\begin{figure}[h] 
\includegraphics[scale=1.1]{m4} 
\centering \caption{Zdania wygenerowane w oparciu o maksymalizację prawdopodobieństwa - partie (1-gram)}
\end{figure}

Dla modeli 2-gramowych zdania są nieco bardziej zróżnicowane, ponieważ im dłuższe są n-gramy, na których opiera się model, tym mniejsza szansa, że będą się powtarzać w różnych korpusach równie często. Dalej jednak są dość podobne. \\
\begin{figure}[!ht] 
\includegraphics[scale=1.1]{m5} 
\centering \caption{Zdania wygenerowane w oparciu o maksymalizację prawdopodobieństwa - partie (2-gram)}
\end{figure}

Konstrukcja	ciągów wyrazów może zatem mieć więcej zastosowań w przypadkach, gdy jednocześnie spełniony jest warunek dotyczący większej ilości słów w korpusie i zadane są odpowiednie warunki brzegowe pozwalające na zróżnicowanie wyników.

\chapter{Dalszy processing danych}
Do kolejnych etapów analiz konieczne było rozszerzenie zakresu danych. Na danych wykonano lematyzację, wektoryzację oraz dodano oznaczenie sentymentu.

\section{Lematyzacja} \label{section:lem}
Do tego etapu pracy z danymi wykorzystano pakiet pythonowy stanza \url{https://stanfordnlp.github.io/stanza/index.html#about}. Jest to pakiet, który może być wykorzystywany m.in. do tokenizacji tekstu, lematyzacji słów, określania części mowy oraz zadań z obszaru NER (Named Entity Recognition).  Modele w pakiecie obejmują ponad 70 języków, w tym polski. Bazują na projekcie Universal Dependencies, którego celem jest spójny opis gramatyk dla różnych języków, który jest rozwijany w celu trenowania rozwiązań dla wielu języków jednocześnie.
W niniejszej pracy wykorzystano modele oparte o korpus LFG, który zawiera 17 246 zdań i 130 967 tokenów. Zawiera oznaczenia 15 części mowy: ADJ, ADP, ADV, AUX, CCONJ, DET, INTJ, NOUN, NUM, PART, PRON, PROPN, PUNCT, SCONJ, VERB.\\
Przykładowe wywołanie dla zdania „Pisanie pracy zaliczeniowej":\\
      id: 1,\\
      text: Pisanie,\\
      lemma: pisać,\\
      upos: NOUN,\\
      xpos: ger:sg:nom:n:imperf:aff,\\
      feats: Aspect=Imp|Case=Nom|Gender=Neut|Number=Sing|Polarity=Pos|VerbForm=Vnoun,\\
      head: 0,\\
      deprel: root,\\
      misc: start\_char=0|end\_char=7\\\\

      id: 2,\\
      text: pracy,\\
      lemma: praca,\\
      upos: NOUN,\\
      xpos: subst:sg:gen:f,\\
      feats: Case=Gen|Gender=Fem|Number=Sing,\\
      head: 1,\\
      deprel: nmod,\\
      misc: start\_char=8|end\_char=13\\

      id: 3,\\
      text: zaliczeniowej,\\
      lemma: zaliczeniowy,\\
      upos: ADJ,\\
      xpos: adj:sg:gen:f:pos,\\
      feats: Case=Gen|Degree=Pos|Gender=Fem|Number=Sing,\\
      head: 2,\\
      deprel: amod,\\
      misc: start\_char=14|end\_char=27\\

Bazując na powyższym pakiecie oraz liście polskich stopwords, przygotowano nową wersję tesktów przemówień, która: \begin{itemize}
\item nie zawiera znaków interpunkcyjnych,
\item składa się z lematów słów
\item nie zawiera słów z listy stopwords (lista rozszerzona o \textit{zł, pkt, art, ustawy, r}).
\end{itemize}

Po wykonaniu lematyzacji przemówień, liczba różnych słów w korpusie spadła z 367 tys. do 152 755 czyli ponad o połowę. Nieco ponad 101 tys. słów występuje mniej niż 5 razy.
Najczęściej występujące słowa przedstawione są w tabeli \ref{tab:najcz}.
\begin{table}[h] \centering
\begin{tabular}{|c|c|} 
\hline
'prawo' & 221 153 \\ \hline
 'komisja' & 227 835 \\ \hline
 'minister' & 236 987 \\ \hline
 'móc' & 241 134 \\ \hline
 'polski' & 245 334 \\ \hline
 'państwo' & 261 503 \\ \hline
 'projekt' & 310 992 \\ \hline
 'rok' & 392 826 \\ \hline
 'mieć' & 521 144 \\ \hline
 'ustawa' & 531 702 \\ \hline \end{tabular} \caption{Najczęściej występujące słowa po lematyzacji} \label{tab:najcz}
\end{table}

%-------------------------------------------------- przypisanie sentymentu
\section{Przypisanie sentymentu} \label{section:sent}







%--------------------------------------------------modelowanie tematyczne

\chapter{Modelowanie tematyczne}



Modelowanie tematyczne (ang. \textit{topic modeling}) jest dziedziną, która skupia się na wykrywaniu tematów w zbiorze dokumentów. Bazuje na stowarzyszeniu określonych słów z tymi tematami. To zagadnienie może występować jako przykład uczenia nadzorowanego. Będzie tak w przypadku, gdy posiadamy zestaw dokumentów z przypisanymi do nich tematami. Wówczas staramy się zbudować model, który nauczy się rozpoznawać te tematy w nowych dokumentach. Z uczeniem nienadzorowanym mamy do czynienia w przypadku, gdy dopiero chcemy odkryć strukturę danego zestawu dokumentów. Nie wiemy jakie tematy mogą się tam pojawiać i stosujemy różne narzędzia statystyczne aby je zidentyfikować.
Jednym z podejść, które można zastosować w takiej analizie jest schemat \textit{tf-idf} (od ang. \textit{term frequency – inverse document frequency}). Polega on na redukcji wymiarowości zbioru danych poprzez konstrukcję macierzy, w której dla każdego dokumentu w~zbiorze przypisujemy wektor bazujący na częstościach występowania w nim określonych słów. Wartość TF-IDF oblicza się ze wzoru:

\begin{equation} 
(tf -idf)_{i,j} =tf_{i,j} \times idf_{i},
\end{equation}
gdzie $tf_{i, j}$ oznacza „term frequency” i wyraża się wzorem:
\begin{equation}
tf_{i,j} =\frac {n_{i,j}}{\sum _{k}n_{k,j}},
\end{equation}
gdzie: $n_{i,j}$ jest liczbą wystąpień słowa $w_{i}$ w dokumencie $d_{j}$ a mianownik jest sumą liczby wystąpień wszystkich słów w dokumencie $d_{j}$. $idf_{i}$ to „inverse document frequency” wyrażana wzorem:
\begin{equation}
idf_{i} =\log \frac {|D|}{|\{d:w_{i}\in d\}|},
\end{equation}
gdzie: \\
$|D|$ – liczba dokumentów w korpusie,\\
$|\{d:w_{i}\in d\}|$ – liczba dokumentów zawierających przynajmniej jedno wystąpienie danego słowa.\\
Otrzymujemy w ten sposób wektory równych długości, które można wykorzystać do analizy podobieństwa dokumentów. Algorytm ten jest stosowany m.in. w wyszukiwarkach internetowych oraz systemach antyplagiatowych. \\
Aby zaadresować potrzebę jeszcze większej redukcji wymiarów, pojawiły się inne metody opisywania dokumentów. Jedną z popularniejszych jest latent Dirichlet allocatioc (LDA) przedstawiona w \cite{LDA}. Zakłada, że każdy dokument jest mieszanką różnych ukrytych (\textit{latent}) tematów i dla każdego tematu istnieje rozkład prawdopodobieństwa słów w nim występujących.\\
Proces generatywny dla dokumentów, zgodnie z LDA wygląda w następujący sposób (z pewnymi uproszczeniami):
\begin{enumerate}
\item Wybierz $N \sim Poisson(\xi)$.
\item Wybierz $\theta \sim Dir(a)$.
\item Dla każdego z $N$ słów $w_n$:
\begin{enumerate}
	\item Wybierz temat $z_n \sim$  Multinomial($\theta$).
	\item Wybierz słowo $w_n$ z rozkładu wielomianowego zależnego od tematu $z_n$, tj. $p(w_n|z_n;\beta)$.
\end{enumerate} \end{enumerate}
Zmienną $\theta$ należy utożsamiać z pewnym podziałem dokumentu na tematy. Przykładowo jeśli rozważamy trzy tematy, może ona przybrać wartość (0.5, 0.2, 0.3). Oznacza to, że połowa słów powinna być związania z tematem nr 1, 20\% z tematem nr 3 i 30\% z tematem 3. Gęstość $k$-wymiarowej zmiennej losowej o rozkłądzie Dirichleta opisuje wzór:
\begin{equation}
p(\theta|\alpha) = \frac {\Gamma (\sum_{i=1}^{k}\alpha_i)} {\prod_{i=1}^{k}\Gamma(\alpha_i)} \theta_1^{\alpha_1 -1} \dots \theta_k^{\alpha_k -1},
\end{equation}
gdzie $\alpha$ jest $k$-wymiarowym wektorem z parametrami $\alpha_i$>0, a $\Gamma$ oznacza funkcję Gamma.\\
Wówczas prawdopodobieństwo całego korpusu $D$ o liczbie dokumentów $M$ wynosi:

\begin{equation}
p(D|\alpha,\beta) = \prod_{d=1}^M \int p(\theta_d |\alpha) \left(\prod_{n=1}^{N_d} \sum_{z_{dn}} p(z_{dn}|\theta_d)p(w_{db}|z_{dn}, \beta) \right) d\theta_d.
 \end{equation}
Warto zauważyć, że parametry $\alpha$ i $\beta$ są właściwe dla całego korpusu. Parametr $\theta_d$ jest losowany dla każdego dokumentu, a zmienne $z_{dn}$ i $w_{dn}$ występują na poziomie słów.

\section{Latent Dirichlet Allocation z wykorzystaniem \textit{bag of words}}
Do analiz związanych z analizą tematu wykorzystano pythonową bibliotekę gensim. 
Aby uzyskać listę tematów obecnych w badanych dokumentach w pierwszej kolejności należy przygotować słownik zawierający wszystkie słowa z badanego korpusu. Tę operację wykorzystujemy na maksymalnie oczyszczonych danych. Funkcja \textit{ gensim.corpora.Dictionary} buduje słownik mapowań konkretnych słów na ich liczbowe odpowiedniki oraz informację o tym ile razy dane słowo występuję w dokumentach i ile dokumentów je zawiera. Opierając się o te parametry, dobrze jest przefiltrować słownik. Z jednej strony należy usunąć słowa pojawiające się zbyt rzadko, ponieważ nie będą one w stanie określić tematu pojawiającego się w wielu dokumentach, a z drugiej strony trzeba również usunąć te, które pojawiają się zbyt często, aby nie zaburzały konstrukcji tematów. Częstą praktyką jest dołożenie do słownika bigramów wygenerowanych w oparciu o korpus, ale z ograniczeniem na częstość występowania.
Słownik zbudowany w taki sposób na naszym korpusie zawiera przed oczyszczeniem 162 726 elementów. Po zastosowaniu domyślnych parametrów eliminacji słów pozostało 62 004. Domyślne parametru usuwają słowo jeśli pojawia się w mniej niż 5 dokumentach lub w więcej niż połowie z nich.
Dla tak przygotowanego słownika możemy zamienić dokumenty na format \textit{bag of words} (BoW), to jest zbiór par $(id, licznik\_id)$ wszystkich słów jakie się w nim pojawiają. Przykładowo da wybranego dokumentu, z tego formatu można uzyskać następującą informację:\\
\textit{Słowo 10 (cywilny) pojawia się 2 razy.\\
Słowo 11 (czas) pojawia się 1 raz.\\
Słowo 14 (człowiek) pojawia się 1 raz.\\
Słowo 38 (istotny) pojawia się 1 raz.\\
Słowo 50 (kraj) pojawia się 1 raz.\\
Słowo 62 (musieć) pojawia się 3 razy.\\
Słowo 64 (myśleć) pojawia się 2 razy.\\
Słowo 83 (obcy) pojawia się 1 raz.\\
Słowo 89 (oczywiście) pojawia się 2 razy.\\
Słowo 99 (okres) pojawia się 1 raz.\\
Słowo 106 (państwo) pojawia się 4 razy.\\
Słowo 107 (pewien) pojawia się 4 razy.\\
Słowo 121 (polityczny) pojawia się 7 razy.}

Następnie możemy przygotować model \textit{tf-idf}, który wyznaczy macierz tych wartości. Zauważmy, że informacje z BoW agregują słowa wyłącznie na poziomie lokalnym, a TF-IDF dodaje do tego wymiar globalny. Sprawdźmy czy modele LDA budowane na tych dwóch zestawach danych będą się od siebie istotnie różnić. Wykorzystamy do tego funkcję \textit{gensim.models.LdaMulticore}. Wywołując tę funkcję możemy określić parametry $num\_topics$ czyli liczbę tematów jakie chcemy określić w naszym korpusie. Domyślna wartość to 100.

Po wywołaniu funkcji z parametrami $ num\_topics=100, passes=1$ otrzymujemy wyniki pokazane na obrazku \ref{tm01}. Wizualizacja prezentuje 10 słów najistotniejszych w określaniu danego tematu. 

\begin{figure}
\includegraphics[scale=0.6]{topics01}
\centering  
\caption{LDA oparte o BOW - 100 tematów}
\label{tm01}
\end{figure}
Niektóre tematy wydają się bardzo precyzyjnie określone np. Topic 89 dotyczy zatrudnienia, Topic 61 rolnictwa, Topic 83 to szkolnictwo a Topic 10 służby porządkowe. Pojawiają się też tematy „mieszane”, którym trudno opisać np. Topic 37. W jego definicji pojawiają się takie słowa jak „osoba”, „państwo” czy „sprawa”, które wydają zbyt pospolite aby dobrze określać poszczególne tematy. Statystyki dla tych słów wyglądają następująco:\\
Słowo 529 - osoba pojawia się w korpusie 132 022 razy i występuje w 47 402 dokumentach (29.8\%).\\
Słowo 181 - sprawa pojawia się w korpusie 202 354 razy i występuje w 73 298 dokumentach (46.1\%).\\
Słowo 107 - państwo pojawia się w korpusie 261 503 razy i występuje w 78 920 dokumentach (49.7\%).

\begin{figure}
\includegraphics[scale=0.6]{topics02} 
\centering  
\caption{LDA oparte o BOW - zmiana filtrowania}
\label{tm02}
\end{figure}
Wykonano ponownie proces oczyszczania słownika ze zmianą parametru $no\_above$ na 30\%. Zmieniło to wielkość słownika jedynie o kilkadziesiąt elementów. Częściowe wyniki zaprezentowane na rysunku \ref{tm02}. W ocenie zasadności zmiany parametru będziemy chcieli wesprzeć się jednak konkretną miarą.

\section{Koherentność modelu}\label{section:ldacoh}
Jeśli trenujemy model korzystając z metod uczenia nienadzorowanego, ocena jego jakości może stanowić wyzwanie. Odnosząc to do zagadnienia identyfikacji tematów, musimy spróbować dobrać miarę, która pozwoli nam decydować o tym, jaka liczba tematów jest ‘lepsza’ dla naszego korpusu lub też jak zmiana parametrów treningu wpływa na dopasowanie modelu. Zagadnienie to jest szeroko analizowane w \cite{RTL}. Autorzy wskazują, że w ocenie takich modeli ważna jest zarówno spójność poszczególnych tematów jaki możliwość ich interpretacji przez ludzi. Wygenerowanie zestawu tematów, które kategoryzują dokumenty, ale nie dają się opisać, nie wnosi wartości dodanej do tego zagadnienia. Opisane przez nich badanie porównywały wyniki kilku metod identyfikacji tematów z wynikami otrzymanymi w badaniu z uczestnictwem ludzi. Uczestnicy badania wykonywali dwa zadania. Pierwsze z nich \textit{word intruder} polegało na wskazaniu niepasującego słowa w podanym zestawie, a drugie \textit{topic intruder} na wskazaniu tematu, który nie pasował do zaprezentowanego fragmentu dokumentu. Otrzymane wyniki były wykorzystane do oceny spójności słów opisujących tematy zwracane przez model praz poprawność w identyfikacji tematów występujących w danym dokumencie. 

W \cite{COH} autorzy przeanalizowali szereg miar dotyczących oceny koherentności (\textit{ang. coherency}) modeli, które pozwalają ocenić spójność generowanych tematów. Proponują stosowanie miary będącej pewnym układem następujących elementów: 

\begin{itemize}
\item metody podziału zestawu słów na podzbiory (segmentation $\mathcal{S}$),
\item miary, za pomocą której będzie określania spójność wygenerowanych podzbiorów (measures $\mathcal{M}$),
\item metody określania prawdopodobieństwa słów, które jest wykorzystywane w powyższych miarach ($\mathcal{P}$),
\item funkcji agregującej otrzymane wyniki ($\Sigma$).
\end{itemize}
Pozwala to zdefiniować przestrzeń możliwych configuracji $ C = \mathcal{S} \times \mathcal{M} \times \mathcal{P}$. W tak zdefiniowanej przestrzeni poszukiwano układu najlepiej korelującego z ludzkim osądem. Miara optymalizująca to zagadnienie została oznaczona jako $C_v$ i jest zaimplementowana w pakiecie $gensim$. 

W kolejnym etapie prac przeprowadzono testy mające na celu ustalenie optymalnego zestawu parametrów do budowy modelu. Parametry, które podlegały zmianie to $no\_above$ (10\%, 20\%, 30\%, 40\%, 50\%) w procesie tworzenia słownika, liczba tematów poszukiwanych w korpusie (20, 30, 40, 50) oraz $passes$ (3, 5, 7, 10) czyli liczba iteracji algorytmu budującego tematy. Zapisano wartość $C_v$ dla każdego układu tych parametrów. Otrzymano wartości z zakresu 0.45 - 0.63 a~ średnia wyniosła 0.55. W tabeli \ref{gs01} zaprezentowano wyniki dla różnych układów parametrów. Aby przyśpieszyć otrzymanie wyniku, modele uczono na zbiorze zawierającym losowo wybrane 30\% korpusu.

\begin{table}  \centering
\begin{tabular}{ |p{2cm}||p{2cm}|p{2cm}|p{2cm}|p{2cm}||p{2cm}| }
 \hline
      &  \multicolumn{4}{|c||}{liczba tematów} &    \\

 \hline
 n\_above & 20 & 30 & 40 & 50 & total \\
 \hline
0.1	&0.61	&0.61	&0.61	&0.60 	&0.61 \\
0.2	&0.55	&0.57	&0.58	&0.58	&0.57\\
0.3	&0.53	&0.53	&0.53	&0.54	&0.53\\
0.4	&0.48	&0.50	&0.53	&0.52	&0.51\\
 \hline \hline
passes & 20 & 30 & 40 & 50 & total \\
\hline
3	&0.52	&0.52	&0.54	&0.54	&0.53\\
5	&0.54	&0.55	&0.56	&0.56	&0.55\\
7	&0.55	&0.57	&0.56	&0.57	&0.56\\
10	&0.56	&0.56	&0.58	&0.58	&0.57\\
\hline \hline
total &	0.54	&0.55	&0.56	&0.56	&0.55\\
\hline
\end{tabular} \caption{Wyniki grid search dla LDA opartego o BOW} \label{gs01}
\end{table}

Wyniki pokazują, że zwiększanie liczby tematów powyżej 40 nie daje poprawy. Wyższe wartości daje natomiast zwiększanie liczby iteracji oraz obniżanie progu eliminacji słów ze słownika. Najwyższą wartość coherence uzyskano dla poziomu eliminacji 0.1 co oznacza, że w budowie modelu brały udział tylko słowa, które pojawiły się w nie więcej niż 10\% dokumentów. Pojawia się wątpliwość czy tak żadnie słowa mogą dobrze generalizować tematy? Czy tematy wygenerowane w takim modelu łatwo interpretować? 

Aby odpowiedzieć na to pytanie zbudowano dwa modele na całym korpusie. Jeden z parametrami (n\_above=0.1, passes=10, topics=40) oraz drugi (n\_above=0.2, passes=10, topics=40) i subiektywnie oceniono interpretowalność tematów na podstawie kluczowych słów. Jako łatwiejszy do opsania wybrano model z paramterem 0.2. Dwa tematy zgrupowano razem jako nie dające sobie przypisać szczególnej etykiety. Model z tymi parametrami zbudowanh na całym korpusie ma coherence równe 0.597.

Na rysunku \ref{pic:lda1} przedstawiono kluczowe słowa dla tematów wraz z opisami.


\begin{figure}
\includegraphics[scale=0.74]{lda_bow} 
\centering \caption{LDA oparte o BOW - tematy}
\label{pic:lda1}
\end{figure}



\section{Wizualizacja i rozkład tematów}\label{section:ldavis}

Wizualizacja modelu może być bardzo pomocna przy ocenie jego jakości. W pierwszej kolejności zweryfikujmy jak wygląda rozkład tematów w naszym korpusie. Aby to zrobić należy do każdej wypowiedzi przypisać jej główny temat. Model LDA zwraca do każdego dokumentu listę tematów oraz ich score oznaczający na ile ten temat opisuje dany dokument. Na rysunku \ref{top_dist01} przedstawiono rozkład tych tematów w korpusie. Widzimy, że najwięcej dokumentów przypisało się do tematów, które trudno było określić jednym słowem i zostały złączene w jeden ogólny. Kolejny na liście to „posiedzenie”. Tym słowem określano temat dotyczący samych obrad, ustaleń i poprawek. Poza tymi, dwa najczęściej się pojawiające się to ustrój państwa, przepisy i bidżet. W pozostałych tematach nie widać już ewidentnie wyróżniających się elementów. Najrzadziej występujące to "kwalifikacje zawodowe" oraz "energia". Można też niestety zauważyć, że tematy związane ze środowiskiem naturlanym poajwiały się bardzo rzadko.
\begin{figure}
\includegraphics[scale=0.4]{tematy01} 
\centering \caption{Rozkład tematów w korpusie (LDA BOW)}
 \label{top_dist01}
\end{figure}

Przydatnym narzędziem do wizualizacji zidentyfikowanych tematów jest pakiet $pyLDAvis$. Wykorzystuje on algorytm PCA do przedstawienia podobieństwa pomiędzy tematami, tzw. Intertopic Distance Map. Dobrze określona grupa tematów powinna na takiej wizualizacji składać się z kółek podobnej wielkości, które na siebie nie nachodzą w zbyt dużym stopniu. Ponadto prezentowane są słowa najbardziej charakterystyczne dla danego tematu. Wizualizacja modelu opartego o BOW znajduje się na rysunku \ref{vis01}.

\begin{figure}
\includegraphics[scale=0.5]{vis01} 
\includegraphics[scale=0.5]{vis02} 
\centering \caption{Wizualizacja modelu z wykorzystaniem pakietu pyLDAvis}
 \label{vis01}
\end{figure}

Wnioski wynikające z tej analizy są spójne z dotychczasowymi. Największe tematy to temat 'ogólny' oraz temat związany z przeprowadzaniem posiedzeń. Temat 40, niemalże zawierający się w 9, to energia i gospodarka odpowiednio.



\section{Latent Dirichlet Allocation w oparciu o TF-IDF}
Jak już było wspomniane, słownik typu Bag of Words zawiera parametry słów wyznaczone lokalnie. Parametry $TF-IDF$ dodają do tego wymiar globalny. Warto więc sprawdzić, czy budowa modelu LDA na takich parametrach pozwoli na wyznaczenie lepszego zbioru tematów.
Dla tego podejścia, analogicznie jak dla Bag Of Words przeszukano przestrzeń parametrów trenowania modelu aby ustalić ich optymalny zestaw. Wyniki różnią się nieco od obserwacji z poprzedniego rozdziału. Najlepszą liczbą tematów wydaje się być 20, chociaż różnice w wartości parametru $coherence$ są niewielkie. Podobnie dla punktu odcięcia $n\_above$. Wyłącznie parametr $passes$ wydaje się różnicować jakość modelu. Ogółem wartości są stabilniejsze i mniej wrażliwe na wartość parametrów niż dla poprzedniej metody. Wyniki testów zawarte są w tabeli \ref{tab:gs03}.


\begin{table}  \centering
\begin{tabular}{ |p{2.5cm}||p{1.8cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}||p{1.8cm}| }
 \hline
      &  \multicolumn{4}{|c||}{liczba tematów} &    \\

 \hline
n\_above & 20 & 30 & 40 & 50 & total \\
 \hline
0.2	 & 0.56 & 	0.54 & 	0.54 & 	0.55 & 0.55\\
0.3	 & 0.54 & 	0.55 & 	0.54 & 	0.56 & 0.55\\
0.4	 & 0.55 & 	0.55 & 	0.55 & 	0.54 & 0.55\\
 \hline \hline
passes & 20 & 30 & 40 & 50 & total \\
\hline
3 & 	0.55 & 	0.54 & 	0.52	 & 0.53	 & 0.54	\\
5 & 	0.54 & 	0.54 & 	0.55	 & 0.55	 & 0.54 \\
7 & 	0.55 & 	0.54 & 	0.56	 & 0.56	 & 0.55\\
10 & 0.56 & 	0.56 & 	0.54 & 	0.55	 & 0.55	\\
\hline \hline
total &		0.55 & 	0.55	 & 0.54 & 	0.55	 & 0.55\\
\hline
\end{tabular} \caption{Wyniki grid search dla LDA opartego o TF-IDF} 
\label{tab:gs03}
\end{table}

\begin{figure}
\includegraphics[scale=0.8]{lda_tf} 
\centering \caption{20 tematów wyodrębionych metodą LDA opartą o TF-IDF}
 \label{lda_tf}
\end{figure}


\begin{figure}
\includegraphics[scale=0.4]{tematy02} 
\centering \caption{Rozkład korpusu względem tematów modelu LDA opartego o TF-IDF}
 \label{top_dist02}
\end{figure}

\begin{figure}
\includegraphics[scale=0.7]{vis03} 
\centering \caption{Wizualizacja modelu z wykorzystaniem pakietu pyLDAvis}
 \label{vis03}
\end{figure}

Ostateczny model zbudowano z wykorzystaniem parametrów $n\_above=0.2$, 20 jako liczba tematów oraz $passes=5$. Coherence tego modelu wynosi 0.64, czyli jest wyże niż modelu opartego tylko o BOW. Opis tematów znajduje się na rysunku \ref{lda_tf}. 

Bazując wyłącznie na parametrze coherence wybralibyśmy ten model jako generujący lepsze przypisanie tematów do wystąpień. Jednak wizulana ocena modelu wykazuje bardzo duże skupienie jedynie w 4 z 20 tematów. Udział wielu jest marginalny. Niesatysfakcjonujący podział potwierdza też wizualizacja pyLDAvis (rysunek \ref{vis03}). W dalszych analizach w rozdziale \ref{models} będzie wykorzystywany model oparty o BOW.








\section{Author-topic model}
Pewnym rozszerzeniem modelowania tematycznego jest dodanie do standardowego LDA wymiaru autora. Takie podejście zostało opisane w \cite{ATM}. Polega ono na rozbudowaniu podstawowego modelu o dodatkowe rozkłady związane z zainteresowaniami autora. Pozwala to na precyzyjny statystyczny opis dokumentów, które mają więcej niż jednego autora. Model generatywny w modelu LDA dla każdego słowa wybiera temat z odpowiednim prawdopodobieństwem a następnie losuje słowa z rozkładu właściwego dla tego tematu. Model \textit{author-topic} dodaje dodatkowy krok, gdzie dla każdego słowa w pierwszej kolejności ustalany jest autor. Dla każdego autora istnieje specyficzny rozkład tematów odzwierciedlający jego zainteresowania, dlatego w zależności od wyboru autora w poprzednim kroku, prawdopodobieństwo pojawienia się danego tematu jest inne. Po ustaleniu tematu losowane jest słowo zgodnie z rozkładem właściwym dla niego.

Zastosowania modeli tego typu są bardzo szerokie. Jednym z wymienionych przez autorów \cite{ATM} jest poszukiwanie recenzentów do prac na podstawie abstraktu. Wymaga to policzenia podobieństwa pomiędzy autorami np. za pomocą symetrycznej dywergencji Kullbacka – Leiblera obliczanej dla rozkładów po tematach.
\begin{equation}
sKL(I,j) = \sum_{t=1}^T \left[ \theta_{it} \log \frac{\theta_{it}}{\theta_{jt}} + \theta_{jt} \log \frac{\theta_{jt}}{\theta_{it}} \right]
\end{equation} 

W tej pracy spróbujemy zastosować modelowanie autorów i tematów do porównywania tematyki wypowiedzi posłów oraz partii.

\section{Author-topic model dla posłów}
W celu analizy podobieństw i różnic pomiędzy tematami pojawiającymi się w wypowiedziach posłów zbudowano model author-topic wykorzystując pakiet \textit{gensim}. Do oczyszczenia słownika zastosowano parametry \textit{no\_above}=0.2, \textit{no\_below}=50 oraz liczby tematów równej 30. Opisano tematy, przy czy dla trzech nie udało się przypisać ogólnego zagadnienia i zostały oznaczone numerami. Przykładowo był to temat nr 4, dla którego kluczowe słowa opisujące to: \textit{(myśleć, wielki, debata, solidarność, program, rada, krajowy, budżet, telewizja, szczególnie}. Niektóre też były dość ogólne dostały więc opisy takie jak \textit{polityka} czy \textit{dyskusja}. Rozkłady tematów dla wybranych polityków znajdują się na rysunku \ref{pic:atm01}.

\begin{figure}
\includegraphics[scale=0.7]{atm01} 
\centering \caption{Rozkłady wypowiedzi względem tematów dla wybranych posłów}
 \label{pic:atm01}
\end{figure}

Konstrukcja takiego modelu pozwala otrzymać w ten sposób wektor o długości 30 opisujący każdego posła. Można wykorzystać tę informację aby zgrupować posłów względem podobieństwa tematów wystąpień i zweryfikować, czy taki podział koreluje z przypisaniem do partii politycznej.
Analizę przeprowadzono dla 5 największych partii tj. Akcja Wyborcza Solidarność, Platforma Obywatelska, Polskie Stronnictwo Ludowe, Prawo i Sprawiedliwość oraz Sojusz Lewicy Demokratycznej. Bazując na metodzie k średnich przypisano każdego z posłów do jednego z 5 klastrów zbudowanych w oparciu o wektory tematów z modelu author-topic. Na wykresie \ref{pic:atm02} widać, że rozkład posłów pomiędzy grupami nie jest jednorodny. Wyraźnie dominuje grupa 2. Tabela \ref{tab:atm01} prezentuje spójność oznaczenia grup i partii w wymiarze liczbowym.

\begin{figure}
\includegraphics[scale=0.5]{atm02} 
\centering \caption{Rozkład posłów względem klastrów}
 \label{pic:atm02}
\end{figure}

\begin{table}[h] \centering
\begin{tabular}{|c|c|c|c|c|c|} 
\hline
Partia / Klaster & 0 & 1 & 2 & 3 & 4  \\ \hline
Akcja Wyborcza Solidarność	 & 17	 &	0	 &	97	 & 6	 & 4\\ \hline
Platforma Obywatelska	 & 55	 & 96	 & 97	 & 13	 & 15\\ \hline
Polskie Stronnictwo Ludowe	 & 23	 & 13	 & 128	 & 30	 & 4\\ \hline
Prawo i Sprawiedliwość	 & 98	 & 76	 & 111	 & 18	 & 14\\ \hline
Sojusz Lewicy Demokratycznej	 & 39	 & 11	 & 216	 & 31	 & 14 \\ \hline 
\end{tabular} \caption{Partia vs wyznaczony klaster} \label{tab:atm01}
\end{table}

Na rysunku \ref{pic:atm03} zwizualizowano tabelę \ref{tab:atm01} jako punkt odniesienia przybierając zarówno wymiar partii jak i przypisania wynikającego z grupowania. Jeśli chodzi o rozkład partii na klastry, Prawo i Sprawiedliwość wygląda podobnie do Platformy Obywatelskiej posiadając o połowę mniejszy udział grupy 2 niż inne partie. Grupa 0 jest najczęściej realizowana przez posłów Prawa i Sprawiedliwości, grupa 1 przez Platformę Obywatelską, grupa 2 przez SLD, podobnie jak grupa 3 ale tutaj duży udział ma również PSL, grupa 4 ma największy udział PO. 

Podsumowując analizy – widoczne są pewne różnice w udziałach grup, ale na pewno nie można stwierdzić, że klasteryzacja odtworzyła podział posłów wynikający z przypisania do partii. Nie można więc powiedzieć, że tematy poruszane przez największe partie różnią się od siebie istotnie udziałem w wypowiedziach ich członków.
Zagadnienie zdefiniowane w tym rozdziale zakłada z~góry liczbę grup równą liczbie partii. Zweryfikowano dodatkowo czy ta wartość jest optymalna jako liczba grup, na które dzieli zbiór metoda k-means. Otóż metoda „łokciowa” sugeruje, że rzeczywista liczba podgrup tematycznych wśród posłów wynosi ok. 15 co widać na rysunku \ref{pic:atm04}. 


\begin{figure}
\includegraphics[scale=0.5]{atm03} 
\centering \caption{Partia vs klaster}
 \label{pic:atm03}
\end{figure}


\begin{figure}[h]
\includegraphics[scale=0.7]{atm04} 
\centering \caption{Wyniki analizy potencjalnej liczby klastrów}
 \label{pic:atm04}
\end{figure}

\chapter{Analiza sentymentu}

Kolejnym zagadnieniem z obszaru przetwarzania tekstu jest analiza sentymentu. Pozwala ona określić nastrój danej wypowiedzi np. charakter recenzji (pozytywna/negatywna) lub prześledzić nastrój książki. Temat ten analizowany jest m.in. w \cite{SA}, gdzie autorzy wskazują różnicę pomiędzy klasyfikacją zawartości tekstu pod względem tematycznym a jego sentymentem. Testowane są tam różne metody, które w oparciu o zmienne będące indykatorami sentymentu, konstruują modele klasyfikujące. Słowa powiązane z sentymentem pozytywnym bądź negatywnym zostały wskazane w trakcie badań.

Mając zbiór treningowy, do którego posiadamy oznaczenie sentymentu poszczególnych elementów, możemy wytrenować zmienne, które najlepiej pozwalają go przewidzieć na nowych danych. W przypadku zbioru wystąpień sejmowych nie istnieją takie etykiety. Należy więc zastosować inną metodę określenia sentymentu. 

\section{Przypisanie sentymentu podstawowym słowom}
W pierwszym kroku wykorzystano listę polskich słów dostępną na stronie Pracowni Obrazowania Mózgu w Instytucie Biologii Doświadczalnej im. M. Nenckiego Polskiej Akademii Nauk (\url{https://exp.lobi.nencki.gov.pl/nawl-analysis}). Lista zawiera ok. 2900 słów wraz z przypisaniem parametrów bliskości do grup: szczęście, złość, smutek, strach, odraza, neutralne. Fragment zbioru prezentuje rysunek \ref{pic:nawl02}. 
\begin{figure}[h]
\includegraphics[scale=0.7]{NAWL02} 
\centering \caption{Fragmnet zbioru z przypisanym sentymentem}
 \label{pic:nawl02}
\end{figure}

Kategoria jest przypisywana, gdy słowo jest jej dostatecznie bliskie jednej z grup i jednocześnie dalekie od innych kategorii. Aby uzyskać przypisanie do jednej pierwszych pięciu kategorii dla jak największej liczby słów, zmodyfikowano parametry graniczne zgodnie z rysunkiem \ref{pic:nawl01} a wszystkim niesklasyfikowanym słowom przypisano kategorię im najbliższą. 

\begin{figure}[h]
\includegraphics[scale=0.7]{NAWL01} 
\centering \caption{Zastosowane parametry do przypisywania grup}
\label{pic:nawl01}
\end{figure}


\begin{table}[h] \centering
\begin{tabular}{|c|c|}
\hline
A & 322\\ \hline
D & 149\\ \hline
F & 465\\ \hline
H & 1740\\ \hline
S & 226\\ \hline
\end{tabular} \caption{Liczność grup z określonym sentymentem} \label{tab:nawl01}
\end{table}
Liczności poszczególnych kategorii po tych modyfikacjach przedstawione są w tabeli \ref{tab:nawl01}.










\section{Przypisanie sentymentu pozostałym słowom}

\section{Sentyment wypowiedzi w czasie oraz w zależności od mówcy i tematu}


\chapter{Łączenie dotychczasowych wyników}\label{models}

\section{Sentyment vs temat }

\section{Klasteryzacja posłów względem rozkładu tematów w modelu author-topic}


%--------------------- 06. BIBLIOGRAFIA ---------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
% Bibliografia leksykograficznie wg nazwisk autorów

\begin{thebibliography}{25}%jak ktoś ma więcej książek, to niech wpisze większą liczbę
% \bibitem[numerek]{referencja} Autor, \emph{Tytuł}, Wydawnictwo, rok, strony
% cytowanie: \cite{referencja1, referencja 2,...}
\bibitem{BH} Peter Hancox, \textit{A brief history of Natural Language Processing}, \url{https://www.cs.bham.ac.uk/~pjh/sem1a5/pt1/pt1_history.html}, dostęp 14/05/2020
\bibitem{NRU} National Research University Higher School of Economics, \textit{Natural Language Processing}, \url{https://www.coursera.org/learn/language-processing/home/info}
\bibitem{LDA} David M. Blei, Andrew Y. Ng and Michael I. Jordan, \textit{Latent Dirichlet Allocation}, Journal of Machine Learning Research 3 (2003) 993-1022
\bibitem{gensim} Selva Prabhakaran, \textit{Topic Modeling with Gensim (Python)}, \url{https://www.machinelearningplus.com/} dostęp 23/05/2020
\bibitem{COH} M. Roeder, A. Both and Alexander Hinneburg, \textit{Exploring the Space of Topic Coherence Measures}, WSDM 2015 - Proceedings of the 8th ACM International Conference on Web Search and Data Mining. 399-408.
\bibitem{RTL} Chang, Jonathan and Sean Gerrish and Wang, Chong and Jordan L. Boyd-graber and David M. Blei, \textit{Reading Tea Leaves: How Humans Interpret Topic Models}, Advances in Neural Information Processing Systems 22, 2009, 288-296
\bibitem{ATM} Michal Rosen-Zvi and Thomas Griffiths and Mark Steyvers and Padhraic Smyth, \textit{The Author-Topic Model for Authors and Documents}, arXiv:1207.4169v1
\bibitem{SA} Bo Pang and Lillian Lee and Shivakumar Vaithyanathan, \textit{Thumbs Up? Sentiment Classification Using Machine Learning Techniques}, 2002 Proceedings of EMNLP 79-86
\bibitem{TM} T. Mikolov; et al. \textit{Efficient Estimation of Word Representations in Vector Space}, 2012	arXiv:1301.3781
\bibitem{TM2} E. Grave, P. Bojanowski, P. Gupta, A. Joulin, T. Mikolov \textit{Learning Word Vectors for 157 Languages}, 2018 arXiv:1802.06893
\end{thebibliography}

\thispagestyle{empty}
\pagenumbering{gobble}


% ----- 8. Spis rysunków - 
\listoffigures
\thispagestyle{empty}
%Jak nie występują, usunąć.


% ------------ 9. Spis tabel - jak wyżej ------------------
\renewcommand{\listtablename}{Spis tabel}
\listoftables
\thispagestyle{empty}
%Jak nie występują, usunąć.


% 10. Spis załączników - 

%\chapter*{Spis załączników}
%\begin{enumerate}
%\item kody\autor.sql

%\end{enumerate}
\thispagestyle{empty}


\end{document}
